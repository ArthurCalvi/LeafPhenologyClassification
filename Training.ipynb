{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from datetime import datetime\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm import tqdm\n",
    "from utils import load_folder, calculate_slope_with_dates\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define the genus and phenology mappings\n",
    "genus_mapping = {1: 'oak', 2: 'fir', 3: 'pine', 4: 'douglas', 5: 'chestnut', 6: 'spruce', 7: 'larch', 8: 'poplar', 9: 'ash', 10: 'beech', 11: 'alder', 12: 'birch', 13: 'hornbeam', 14: 'locusts'}\n",
    "phen_mapping = {1: 'deciduous', 2: 'evergreen'}\n",
    "\n",
    "def load_data_from_tile(path: str) -> dict:\n",
    "    tile_id = os.path.basename(path).split('_')[1]\n",
    "    dates = [datetime.strptime(filename.split('_')[0], '%Y-%m-%d') for filename in os.listdir(os.path.join(path, 'rgb'))]\n",
    "    dates.sort()\n",
    "    rgb = load_folder(os.path.join(path, 'rgb'))\n",
    "    chm = rasterio.open(os.path.join(path, 'tree_map', 'CHM2020.tif')).read(1)\n",
    "    forest_mask = (chm > 250).astype(bool)\n",
    "    slope_map = calculate_slope_with_dates(rgb[:, 0], dates, len(rgb[:, 0]) / 2, len(rgb[:, 0])) / 100\n",
    "    weights = (1 - abs(slope_map.ravel())).clip(0, 1)\n",
    "\n",
    "    path_features = os.path.join(path, 'features')\n",
    "    r_APO = rasterio.open(os.path.join(path_features, 'r_APO.tif')).read()\n",
    "    amplitude_map_r, phase_map_r, offset_map_r = r_APO[0], r_APO[1], r_APO[2]\n",
    "    crswir_APO = rasterio.open(os.path.join(path_features, 'crswir_APO.tif')).read()\n",
    "    amplitude_map_crswir, phase_map_crswir, offset_map_crswir = crswir_APO[0], crswir_APO[1], crswir_APO[2]\n",
    "    rcc_APO = rasterio.open(os.path.join(path_features, 'rcc_APO.tif')).read()\n",
    "    amplitude_map_rcc, phase_map_rcc, offset_map_rcc = rcc_APO[0], rcc_APO[1], rcc_APO[2]\n",
    "    dem = rasterio.open(os.path.join(path_features, 'elevation_aspect.tif')).read()\n",
    "    elevation, aspect = dem[0], dem[1]\n",
    "\n",
    "    features = {\n",
    "        'amplitude_red': amplitude_map_r.ravel(),\n",
    "        'phase_red': phase_map_r.ravel(),\n",
    "        'offset_red': offset_map_r.ravel(),\n",
    "        'amplitude_crswir': amplitude_map_crswir.ravel(),\n",
    "        'phase_crswir': phase_map_crswir.ravel(),\n",
    "        'offset_crswir': offset_map_crswir.ravel(),\n",
    "        'amplitude_rcc': amplitude_map_rcc.ravel(),\n",
    "        'phase_rcc': phase_map_rcc.ravel(),\n",
    "        'offset_rcc': offset_map_rcc.ravel(),\n",
    "        'elevation': elevation.ravel(),\n",
    "        'aspect': aspect.ravel(),\n",
    "        'tile_id': np.array([tile_id] * aspect.size)  # Add tile_id to the features\n",
    "    }\n",
    "\n",
    "    filtered_features = {k: v[forest_mask.ravel()] for k, v in features.items()}\n",
    "    filtered_weights = weights[forest_mask.ravel()]\n",
    "\n",
    "    path_reference = os.path.join(path, 'reference_species')\n",
    "    tif = [x for x in os.listdir(path_reference) if x.endswith('.tif')]\n",
    "    reference = rasterio.open(os.path.join(path_reference, tif[0])).read()\n",
    "    genus = reference[1]\n",
    "    phen = reference[2]  # Assuming phenology data is stored in the third band\n",
    "    filtered_genus = genus[forest_mask]\n",
    "    filtered_phen = phen[forest_mask]\n",
    "\n",
    "    filtered_features['genus'] = filtered_genus\n",
    "    filtered_features['phen'] = filtered_phen\n",
    "\n",
    "    return filtered_features, filtered_weights\n",
    "\n",
    "def load_data(directory: str) -> pd.DataFrame:\n",
    "    all_data = []\n",
    "    all_weights = []\n",
    "    tile_to_greco = {}\n",
    "\n",
    "    for folder in tqdm(os.listdir(directory)):\n",
    "        path = os.path.join(directory, folder)\n",
    "        if folder.__contains__('.DS_Store') or folder.__contains__('.txt'):\n",
    "            continue\n",
    "        try:\n",
    "            tile_data, tile_weight = load_data_from_tile(path)\n",
    "            tile_id = os.path.basename(path).split('_')[1]\n",
    "            greco_region = \"_\".join(os.path.basename(path).split('_')[4:-1])\n",
    "            tile_to_greco[tile_id] = greco_region\n",
    "            tile_df = pd.DataFrame(tile_data)\n",
    "            tile_df['tile_id'] = tile_id\n",
    "            all_data.append(tile_df)\n",
    "            all_weights.append(tile_weight)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {folder}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Loaded {len(all_data)} tiles\")\n",
    "    data_df = pd.concat(all_data, ignore_index=True)\n",
    "    weights_array = np.concatenate(all_weights)\n",
    "\n",
    "    return data_df, weights_array, tile_to_greco\n",
    "\n",
    "# Load data\n",
    "data_dir = '/Users/arthurcalvi/Data/species/validation/tiles'\n",
    "data, all_weights, tile_to_greco = load_data(data_dir)\n",
    "\n",
    "# Verify data structure\n",
    "print(data.head())\n",
    "print(tile_to_greco)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming data, all_weights, and tile_to_greco are already loaded from the provided code\n",
    "\n",
    "# Add GRECO region to the data\n",
    "data['greco_region'] = data['tile_id'].map(tile_to_greco)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "def stratified_group_split(data, test_size=0.25, random_state=42):\n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    split = splitter.split(data, groups=data['tile_id'])\n",
    "    train_idx, val_idx = next(split)\n",
    "    \n",
    "    train_data = data.iloc[train_idx]\n",
    "    val_data = data.iloc[val_idx]\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "train_data, val_data = stratified_group_split(data)\n",
    "\n",
    "# Verify the split\n",
    "def verify_split(train_data, val_data):\n",
    "    # Check distribution of phenology classes\n",
    "    train_phen_counts = Counter(train_data['phen'])\n",
    "    val_phen_counts = Counter(val_data['phen'])\n",
    "    \n",
    "    print(\"Training phenology class distribution:\", train_phen_counts)\n",
    "    print(\"Validation phenology class distribution:\", val_phen_counts)\n",
    "    \n",
    "    # Check distribution of GRECO regions\n",
    "    train_greco_counts = Counter(train_data['greco_region'])\n",
    "    val_greco_counts = Counter(val_data['greco_region'])\n",
    "    \n",
    "    print(\"Training GRECO region distribution:\", train_greco_counts)\n",
    "    print(\"Validation GRECO region distribution:\", val_greco_counts)\n",
    "    \n",
    "    # Ensure no overlap of tiles between training and validation\n",
    "    train_tiles = set(train_data['tile_id'])\n",
    "    val_tiles = set(val_data['tile_id'])\n",
    "    \n",
    "    print(\"Common tiles between training and validation:\", train_tiles.intersection(val_tiles))\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_table = pd.DataFrame(columns=['GRECO Region', 'Set', 'Deciduous', 'Evergreen', 'Total'])\n",
    "    \n",
    "    for greco_region in set(train_data['greco_region']).union(set(val_data['greco_region'])):\n",
    "        train_deciduous = len(train_data[(train_data['greco_region'] == greco_region) & (train_data['phen'] == 1)])\n",
    "        train_evergreen = len(train_data[(train_data['greco_region'] == greco_region) & (train_data['phen'] == 2)])\n",
    "        val_deciduous = len(val_data[(val_data['greco_region'] == greco_region) & (val_data['phen'] == 1)])\n",
    "        val_evergreen = len(val_data[(val_data['greco_region'] == greco_region) & (val_data['phen'] == 2)])\n",
    "        \n",
    "        summary_table = summary_table.append({\n",
    "            'GRECO Region': greco_region,\n",
    "            'Set': 'Training',\n",
    "            'Deciduous': train_deciduous,\n",
    "            'Evergreen': train_evergreen,\n",
    "            'Total': train_deciduous + train_evergreen\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "        summary_table = summary_table.append({\n",
    "            'GRECO Region': greco_region,\n",
    "            'Set': 'Validation',\n",
    "            'Deciduous': val_deciduous,\n",
    "            'Evergreen': val_evergreen,\n",
    "            'Total': val_deciduous + val_evergreen\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "    # Add total row\n",
    "    total_deciduous_train = summary_table[summary_table['Set'] == 'Training']['Deciduous'].sum()\n",
    "    total_evergreen_train = summary_table[summary_table['Set'] == 'Training']['Evergreen'].sum()\n",
    "    total_deciduous_val = summary_table[summary_table['Set'] == 'Validation']['Deciduous'].sum()\n",
    "    total_evergreen_val = summary_table[summary_table['Set'] == 'Validation']['Evergreen'].sum()\n",
    "    \n",
    "    summary_table = summary_table.append({\n",
    "        'GRECO Region': 'Total',\n",
    "        'Set': 'Training',\n",
    "        'Deciduous': total_deciduous_train,\n",
    "        'Evergreen': total_evergreen_train,\n",
    "        'Total': total_deciduous_train + total_evergreen_train\n",
    "    }, ignore_index=True)\n",
    "    \n",
    "    summary_table = summary_table.append({\n",
    "        'GRECO Region': 'Total',\n",
    "        'Set': 'Validation',\n",
    "        'Deciduous': total_deciduous_val,\n",
    "        'Evergreen': total_evergreen_val,\n",
    "        'Total': total_deciduous_val + total_evergreen_val\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    mapping_real_greco = {'Côtes_et_plateaux_de_la_Manche': 'Centre Nord semi-océanique',\n",
    "                      'Côtes_et_plateaux_de_la_Manche': 'Centre Nord semi-océanique',\n",
    "                      'Ardenne_primaire': 'Grand Est semi-continental',\n",
    "                      'Préalpes_du_Nord': 'Alpes',\n",
    "                      'Préalpes_du_Nord': 'Alpes',\n",
    "                      'Garrigues' : 'Méditerranée',\n",
    "                      'Massif_vosgien_central': 'Vosges',\n",
    "                        'Premier_plateau_du_Jura': 'Jura',\n",
    "                        'Piémont_pyrénéen' : 'Pyrénées',\n",
    "                        'Terres_rouges': 'Sud-Ouest océanique' ,\n",
    "                          'Corse_occidentale': 'Corse',\n",
    "                        \"Châtaigneraie_du_Centre_et_de_l'Ouest\": 'Massif central' ,\n",
    "                        'Ouest-Bretagne_et_Nord-Cotentin': 'Grand Ouest cristallin et océanique', \n",
    "                        'Total': 'Total'\n",
    "\n",
    "}\n",
    "\n",
    "    summary_table['GRECO Region'] = summary_table['GRECO Region'].map(mapping_real_greco)\n",
    "    \n",
    "    print(summary_table)\n",
    "    summary_table.to_csv('summary_table.csv', index=False)\n",
    "    return summary_table\n",
    "\n",
    "# Verify the split\n",
    "summary_table = verify_split(train_data, val_data)\n",
    "\n",
    "# Save the datasets\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "val_data.to_csv('val_data.csv', index=False)\n",
    "\n",
    "print(\"Training and validation datasets saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_valid, y_phen_valid, test_size=0.5, random_state=42)\n",
    "\n",
    "# Look at distribution of phenological stages (1 = deciduous, 2 = evergreen) in training and validation sets \n",
    "# PLOT DISTRIBUTION OF PHENOLOGICAL STAGES IN TRAINING AND VALIDATION SETS AS A BAR PLOT\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 3))\n",
    "y_train.value_counts().plot(kind='bar', ax=ax[0], title='Training set')\n",
    "y_test.value_counts().plot(kind='bar', ax=ax[1], title='Validation set')\n",
    "#SET TICK LABELS\n",
    "ax[0].set_xticklabels(['Deciduous', 'Evergreen'])\n",
    "ax[1].set_xticklabels(['Deciduous', 'Evergreen'])\n",
    "#put tick label on the bar \n",
    "ax[0].tick_params(axis='x', rotation=0)\n",
    "ax[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "#set as percentage \n",
    "ax[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '{:.0%}'.format(x/len(y_train))))\n",
    "ax[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '{:.0%}'.format(x/len(y_test))))\n",
    "#set color evergreen as green and deciduous as brown\n",
    "ax[0].get_children()[0].set_color('brown')\n",
    "ax[0].get_children()[1].set_color('green')\n",
    "ax[1].get_children()[0].set_color('brown')\n",
    "ax[1].get_children()[1].set_color('green')\n",
    "\n",
    "#add horizontal grid lines\n",
    "ax[0].yaxis.grid(True, linestyle='--', alpha=0.5)\n",
    "ax[1].yaxis.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "#print in term of counts\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Train Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = rf.feature_importances_\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importance})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatial validation ensures that your model generalizes well across different forest habitats. It involves training and testing the model on different spatial units. Here, we'll use a leave-one-out approach for different tiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils import load_folder, calculate_slope_with_dates\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define the genus mapping\n",
    "genus_mapping = {\n",
    "    1: 'oak', 2: 'fir', 3: 'pine', 4: 'douglas', 5: 'chestnut',\n",
    "    6: 'spruce', 7: 'larch', 8: 'poplar', 9: 'ash', 10: 'beech',\n",
    "    11: 'alder', 12: 'birch', 13: 'hornbeam', 14: 'locusts'\n",
    "}\n",
    "\n",
    "phen_mapping = {1: 'deciduous', 2: 'evergreen'}\n",
    "\n",
    "# Load the data and preprocess it\n",
    "def load_data_from_tile(path: str) -> dict:\n",
    "    dates = [datetime.strptime(filename.split('_')[0], '%Y-%m-%d') for filename in os.listdir(os.path.join(path, 'rgb'))]\n",
    "    dates.sort()\n",
    "    rgb = load_folder(os.path.join(path, 'rgb'))\n",
    "    chm = rasterio.open(os.path.join(path, 'tree_map', 'CHM2020.tif')).read(1)\n",
    "    forest_mask = (chm > 250).astype(bool)\n",
    "    slope_map = calculate_slope_with_dates(rgb[:, 0], dates, len(rgb[:, 0]) / 2, len(rgb[:, 0])) / 100\n",
    "    weights = (1 - abs(slope_map.ravel())).clip(0, 1)\n",
    "\n",
    "    path_features = os.path.join(path, 'features')\n",
    "    r_APO = rasterio.open(os.path.join(path_features, 'r_APO.tif')).read()\n",
    "    amplitude_map_r, phase_map_r, offset_map_r = r_APO[0], r_APO[1], r_APO[2]\n",
    "    crswir_APO = rasterio.open(os.path.join(path_features, 'crswir_APO.tif')).read()\n",
    "    amplitude_map_crswir, phase_map_crswir, offset_map_crswir = crswir_APO[0], crswir_APO[1], crswir_APO[2]\n",
    "    rcc_APO = rasterio.open(os.path.join(path_features, 'rcc_APO.tif')).read()\n",
    "    amplitude_map_rcc, phase_map_rcc, offset_map_rcc = rcc_APO[0], rcc_APO[1], rcc_APO[2]\n",
    "    dem = rasterio.open(os.path.join(path_features, 'elevation_aspect.tif')).read()\n",
    "    elevation, aspect = dem[0], dem[1]\n",
    "\n",
    "    features = {\n",
    "        'amplitude_red': amplitude_map_r.ravel(),\n",
    "        'phase_red': phase_map_r.ravel(),\n",
    "        'offset_red': offset_map_r.ravel(),\n",
    "        'amplitude_crswir': amplitude_map_crswir.ravel(),\n",
    "        'phase_crswir': phase_map_crswir.ravel(),\n",
    "        'offset_crswir': offset_map_crswir.ravel(),\n",
    "        'amplitude_rcc': amplitude_map_rcc.ravel(),\n",
    "        'phase_rcc': phase_map_rcc.ravel(),\n",
    "        'offset_rcc': offset_map_rcc.ravel(),\n",
    "        'elevation': elevation.ravel(),\n",
    "        'aspect': aspect.ravel()\n",
    "    }\n",
    "\n",
    "    filtered_features = {k: v[forest_mask.ravel()] for k, v in features.items()}\n",
    "    filtered_weights = weights[forest_mask.ravel()]\n",
    "\n",
    "    path_reference = os.path.join(path, 'reference_species')\n",
    "    tif = [x for x in os.listdir(path_reference) if x.endswith('.tif')]\n",
    "    reference = rasterio.open(os.path.join(path_reference, tif[0])).read()\n",
    "    genus = reference[1]\n",
    "    phen = reference[2]  # Assuming phenology data is stored in the third band\n",
    "    filtered_genus = genus[forest_mask]\n",
    "    filtered_phen = phen[forest_mask]\n",
    "\n",
    "    filtered_features['genus'] = filtered_genus\n",
    "    filtered_features['phen'] = filtered_phen\n",
    "\n",
    "    return filtered_features, filtered_weights\n",
    "\n",
    "def load_data(directory: str) -> pd.DataFrame:\n",
    "    all_data = []\n",
    "    all_weights = []\n",
    "\n",
    "    for folder in tqdm(os.listdir(directory)):\n",
    "        path = os.path.join(directory, folder)\n",
    "        if folder.endswith('validation') or folder.__contains__('.DS_Store') or folder.__contains__('.txt'):\n",
    "            continue\n",
    "        try:\n",
    "            tile_data, tile_weight = load_data_from_tile(path)\n",
    "            all_data.append(pd.DataFrame(tile_data))\n",
    "            all_weights.append(tile_weight)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {folder}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.concat(all_data, ignore_index=True), np.concatenate(all_weights)\n",
    "\n",
    "# Load data\n",
    "data_dir = '/Users/arthurcalvi/Data/species/validation/tiles'\n",
    "data, all_weights = load_data(data_dir)\n",
    "\n",
    "# Preprocess data\n",
    "features = ['amplitude_red', 'phase_red', 'offset_red', 'amplitude_crswir', 'phase_crswir', 'offset_crswir', 'amplitude_rcc', 'phase_rcc', 'offset_rcc', 'elevation', 'aspect']\n",
    "X = data[features]\n",
    "y_phen = data['phen']\n",
    "\n",
    "# Replace infinite values with NaNs\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Handle NaNs by imputing with the mean of each column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Update X with imputed values\n",
    "X = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "# Filter out null data in y_phen\n",
    "valid_mask = y_phen != 0\n",
    "X_valid = X[valid_mask]\n",
    "y_phen_valid = y_phen[valid_mask]\n",
    "\n",
    "# Identify the best feature using ROC-AUC score\n",
    "best_feature = None\n",
    "best_roc_auc = 0\n",
    "roc_auc_list = []\n",
    "\n",
    "for feature in features:\n",
    "    roc_auc = roc_auc_score(y_phen_valid, X_valid[feature])\n",
    "    roc_auc_list.append(roc_auc)\n",
    "    if roc_auc > best_roc_auc:\n",
    "        best_roc_auc = roc_auc\n",
    "        best_feature = feature\n",
    "\n",
    "print(f\"Best feature for distinguishing deciduous and evergreen trees: {best_feature} with ROC-AUC score: {best_roc_auc}\")\n",
    "\n",
    "# Compute the optimal threshold for the best feature\n",
    "fpr, tpr, thresholds = roc_curve(y_phen_valid, X_valid[best_feature], pos_label=2)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Optimal threshold for {best_feature}: {optimal_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Collecting ROC-AUC scores for all features\n",
    "roc_auc_list = []\n",
    "\n",
    "for feature in features:\n",
    "    roc_auc = roc_auc_score(y_phen_valid, X_valid[feature])\n",
    "    roc_auc_list.append(roc_auc)\n",
    "\n",
    "# Plotting\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "\n",
    "# Plotting the ROC curve\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate', color=color)\n",
    "ax1.plot(fpr, tpr, color=color, label=f'ROC curve (area = {best_roc_auc:.2f})')\n",
    "ax1.scatter(fpr[optimal_idx], tpr[optimal_idx], marker='o', color='red', label=f'Optimal Threshold = {optimal_threshold:.2f}')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, linestyle='--', linewidth=0.5)\n",
    "ax1.set_title(f'ROC Curve and AUC-ROC Scores for {best_feature}')\n",
    "\n",
    "# Adding a second y-axis for the histogram\n",
    "color = 'tab:orange'\n",
    "ax2.set_ylabel('AUC-ROC Score', color=color)\n",
    "ax2.bar(features, roc_auc_list, color=color, alpha=0.6)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "ax2.set_xticklabels(features, rotation=45, ha='right')\n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.show()\n",
    "\n",
    "import os \n",
    "os.makedirs('images', exist_ok=True)\n",
    "fig.savefig('images/roc_auc_scores_and_features.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_phen_valid[ y_phen_valid == 2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_phen_valid[ y_phen_valid == 1].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kayrros-default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
