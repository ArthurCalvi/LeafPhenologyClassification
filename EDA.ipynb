{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_and_preprocess_table_data\n",
    "\n",
    "config = \"no_resample_cloud_disturbance_weights_3Y\"\n",
    "data = load_and_preprocess_table_data(config)\n",
    "# Define features and target\n",
    "features = ['amplitude_red', 'cos_phase_red', 'sin_phase_red', 'offset_red',\n",
    "            'amplitude_green', 'cos_phase_green', 'sin_phase_green', 'offset_green',\n",
    "            'amplitude_blue', 'cos_phase_blue', 'sin_phase_blue', 'offset_blue',\n",
    "            'amplitude_crswir', 'cos_phase_crswir', 'sin_phase_crswir', 'offset_crswir', \n",
    "            'elevation', 'sin_aspect', 'cos_aspect']\n",
    "target = 'phen'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of feature and deciduous/evergreen proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import warnings\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Function to compute KDE and color the area under the curve\n",
    "def plot_kde_with_colored_area(data, feature, ax, show_legend=False):\n",
    "    # Data for deciduous and evergreen\n",
    "    data_deciduous = data[data['phen'] == 1][feature]\n",
    "    data_evergreen = data[data['phen'] == 2][feature]\n",
    "\n",
    "    # Calculate KDE\n",
    "    kde_deciduous = gaussian_kde(data_deciduous)\n",
    "    kde_evergreen = gaussian_kde(data_evergreen)\n",
    "    \n",
    "    # X values for KDE\n",
    "    if 'amplitude' in feature or 'offset' in feature:\n",
    "        lower_bound = np.percentile(data[feature], 1)\n",
    "        upper_bound = np.percentile(data[feature], 99)\n",
    "    else : \n",
    "        lower_bound = data[feature].min()\n",
    "        upper_bound = data[feature].max()\n",
    "\n",
    "    x_vals = np.linspace(lower_bound, upper_bound, 1000)\n",
    "    kde_vals_deciduous = kde_deciduous(x_vals)\n",
    "    kde_vals_evergreen = kde_evergreen(x_vals)\n",
    "\n",
    "    # Calculate proportions\n",
    "    total_kde_vals = kde_vals_deciduous + kde_vals_evergreen\n",
    "    prop_deciduous = kde_vals_deciduous / total_kde_vals\n",
    "    prop_evergreen = kde_vals_evergreen / total_kde_vals\n",
    "\n",
    "    # Normalize to get densities\n",
    "    total_kde_vals_percent = total_kde_vals / total_kde_vals.sum()\n",
    "\n",
    "    # Plot KDE\n",
    "    sns.lineplot(x=x_vals, y=total_kde_vals_percent, ax=ax, color='black', linewidth=2)\n",
    "    \n",
    "    if show_legend:\n",
    "        ax.legend(['KDE'], loc='upper right')\n",
    "\n",
    "    # Color the area under the curve\n",
    "    for i in range(len(x_vals) - 1):\n",
    "        ax.fill_between(x_vals[i:i+2], 0, total_kde_vals_percent[i:i+2], color=(prop_deciduous[i], prop_evergreen[i], 0, 0.3))\n",
    "\n",
    "    feature_name = feature.replace('_', ' ').capitalize()\n",
    "    # ax.set_title(f'Distribution of {feature_name}')\n",
    "    ax.set_xlabel(feature_name)\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # Set x-axis limits to 95% of the data\n",
    "    ax.set_xlim(lower_bound, upper_bound)\n",
    "\n",
    "# Plot distributions for overall data using KDE with colored areas\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "idx = 0\n",
    "n_features = len(features)\n",
    "\n",
    "for feature in tqdm(features):\n",
    "    ax = plt.subplot(5, 4, idx + 1)\n",
    "    plot_kde_with_colored_area(data, feature, ax, show_legend=False)\n",
    "    idx += 1\n",
    "\n",
    "ax = plt.subplot(5, 4, idx + 1)\n",
    "#add legend with green patch for evergreen, red patch for deciduous and a line for the KDE\n",
    "handles = [Line2D([0], [0], color='black', linewidth=2, label='KDE'),\n",
    "           Patch(color=(1, 0, 0, 0.3), label='Deciduous'),\n",
    "           Patch(color=(0, 1, 0, 0.3), label='Evergreen')]\n",
    "\n",
    "ax.legend(handles=handles, loc='upper right')\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('images/kde_colored_area.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain -> Mutual Information \n",
    "\n",
    "Information Gain is a metric used to measure the effectiveness of a feature in terms of its contribution to predicting the target variable. It is commonly used in decision trees and other machine learning algorithms to assess the relevance of different features.\n",
    "\n",
    "The concept of Information Gain originates from information theory, where it is defined as the reduction in entropy, or uncertainty, of a target variable given the knowledge of a feature. In simpler terms, it quantifies how much knowing the value of a feature improves our ability to predict the target variable.\n",
    "\n",
    "### Entropy refresher \n",
    "\n",
    "Entropy, denoted as  $H(T)$, is a measure of randomness or disorder in a set of data. It quantifies the impurity or unpredictability of the target variable. Entropy is highest when the probability distribution of the classes is uniform, indicating maximum uncertainty. Conversely, entropy is zero when the dataset is perfectly pure (i.e., all instances belong to a single class).\n",
    "\n",
    "The entropy of a target variable $T$ with $c$ possible classes is calculated as:\n",
    "\n",
    "$$ H(T) = -\\sum_{i=1}^{c} p_i \\log_2(p_i) $$ \n",
    "\n",
    "where:\n",
    "- $p_i$ is the proportion of instances in class $i$.\n",
    "\n",
    "### Information Gain definition \n",
    "\n",
    "Information Gain measures the reduction in entropy after the dataset is split based on a feature. The higher the Information Gain, the more the feature contributes to reducing uncertainty about the target variable.\n",
    "\n",
    "Mathematically, Information Gain $IG(T, X)$ is computed as the difference between the entropy of the target variable before and after observing the feature:\n",
    "\n",
    "$$IG(T, X) = H(T) - H(T | X)$$\n",
    "\n",
    "where:\n",
    "- $H(T)$ is the entropy of the target variable $T$.\n",
    "- $H(T | X)$ is the conditional entropy of $T$ given the feature $X$.\n",
    "\n",
    "The conditional entropy $H(T | X)$ is calculated as:\n",
    "\n",
    "$$H(T | X) = \\sum_{v \\in \\text{Values}(X)} \\frac{|S_v|}{|S|} H(S_v)$$\n",
    "\n",
    "where:\n",
    "- $\\text{Values}(X)$ represents the unique values of the feature $X$.\n",
    "- $S_v$ is the subset of $S$ for which feature $X$ has value $v$.\n",
    "- $|S|$ and $|S_v|$ are the sizes of the sets $S$ and $S_v$, respectively.\n",
    "\n",
    "By calculating the Information Gain for each feature, we can identify which features are the most informative for predicting the target variable. Features with higher Information Gain are considered more important as they contribute more to reducing the uncertainty about the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = data[features]\n",
    "y = data['phen']\n",
    "\n",
    "# Calculate Information Gain (Mutual Information)\n",
    "mi = mutual_info_classif(X, y, discrete_features=False, random_state=42)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "mi_df = pd.DataFrame({'Feature': features, 'Information Gain': mi})\n",
    "mi_df = mi_df.sort_values(by='Information Gain', ascending=False)\n",
    "\n",
    "# Plotting the Information Gain\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "bars = ax.barh(mi_df['Feature'], mi_df['Information Gain'], color='skyblue')\n",
    "ax.set_xlabel('Information Gain')\n",
    "ax.set_ylabel('Features')\n",
    "ax.set_title('Information Gain of Features for Tree Phenology Classification')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Adding vertical grid lines\n",
    "ax.xaxis.grid(True, which='major', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Removing top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Color-coding the y-tick labels\n",
    "def color_code_ticks(ax, labels):\n",
    "    for label in labels:\n",
    "        label_text = label.get_text()\n",
    "        if 'red' in label_text:\n",
    "            label.set_color('red')\n",
    "        elif 'green' in label_text:\n",
    "            label.set_color('green')\n",
    "        elif 'blue' in label_text:\n",
    "            label.set_color('blue')\n",
    "        elif 'crswir' in label_text:\n",
    "            label.set_color('darkred')\n",
    "\n",
    "# Apply color coding to the y-tick labels\n",
    "color_code_ticks(ax, ax.get_yticklabels())\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/information_gain.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table = mi_df.to_latex(index=True, index_names=False)\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher's Score\n",
    "\n",
    "Fisher's Score, also known as Fisher Discriminant Ratio, is a method used in feature selection to evaluate the importance of individual features in distinguishing between different classes. It is based on the concept of maximizing the separation between different classes while minimizing the variation within each class.\n",
    "\n",
    "The Fisher Score for a feature is calculated by taking the ratio of the variance between classes to the variance within classes. Features with higher Fisher Scores are considered more relevant for classification tasks as they provide better discrimination between classes.\n",
    "\n",
    "Mathematically, the Fisher Score $F_i$ for a feature $i$ is defined as:\n",
    "\n",
    "$$F_i = \\frac{\\sum_{c=1}^{C} n_c (\\mu_{i,c} - \\mu_i)^2}{\\sum_{c=1}^{C} n_c \\sigma_{i,c}^2}$$\n",
    "\n",
    "where:\n",
    "- $C$ is the number of classes.\n",
    "- $n_c$ is the number of samples in class $c$.\n",
    "- $\\mu_{i,c}$ is the mean of the feature $i$ in class $c$.\n",
    "- $\\mu_i$ is the mean of the feature $i$ across all classes.\n",
    "- $\\sigma_{i,c}$ is the standard deviation of the feature $i$ in class $c$.\n",
    "\n",
    "By computing the Fisher Scores for each feature, we can rank the features based on their ability to discriminate between classes. This helps in selecting the most informative features for building classification models.\n",
    "\n",
    "In the context of our tree phenology classification task, we will use Fisher's Score to evaluate the importance of various spectral and topographical features in distinguishing between deciduous and evergreen trees. This analysis will guide us in selecting the most relevant features, thereby enhancing the performance of our classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Calculate Fisher Score for each feature\n",
    "def fisher_score(X: pd.DataFrame, y: pd.Series) -> np.ndarray:\n",
    "    \"\"\"Calculate Fisher Score for each feature in X with respect to target y.\"\"\"\n",
    "    classes = np.unique(y)\n",
    "    n_classes = len(classes)\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    fisher_scores = np.zeros(X.shape[1])\n",
    "    \n",
    "    for idx, feature in enumerate(X.columns):\n",
    "        mean_overall = np.mean(X[feature])\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "        \n",
    "        for cls in classes:\n",
    "            X_cls = X[y == cls]\n",
    "            n_cls = X_cls.shape[0]\n",
    "            mean_cls = np.mean(X_cls[feature])\n",
    "            var_cls = np.var(X_cls[feature])\n",
    "            \n",
    "            numerator += n_cls * (mean_cls - mean_overall) ** 2\n",
    "            denominator += n_cls * var_cls\n",
    "            \n",
    "        fisher_scores[idx] = numerator / denominator\n",
    "    \n",
    "    return fisher_scores\n",
    "\n",
    "# Compute Fisher Scores\n",
    "fisher_scores = fisher_score(X, y)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "fisher_df = pd.DataFrame({'Feature': features, 'Fisher Score': fisher_scores})\n",
    "fisher_df = fisher_df.sort_values(by='Fisher Score', ascending=False)\n",
    "\n",
    "# Plotting the Fisher Scores\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.barh(fisher_df['Feature'], fisher_df['Fisher Score'], color='skyblue')\n",
    "ax.set_xlabel('Fisher Score')\n",
    "ax.set_ylabel('Features')\n",
    "ax.set_title('Fisher Score of Features for Tree Phenology Classification')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Removing top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('fisher_score.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comp fisher's vs information gain \n",
    "\n",
    "\n",
    "- **High Scores in Both Methods:** Features with high scores in both Information Gain and Fisher's Score are likely to be highly relevant for the classification task.\n",
    "- **Discrepancies Between Methods:** If a feature has a high score in one method but not the other, this might indicate specific characteristics of the data or the type of relationship between the feature and the target variable. For example, Information Gain might highlight non-linear relationships that Fisher's Score does not capture.\n",
    "- **Feature Selection:** By combining insights from both methods, you can make more informed decisions about which features to include in your model to potentially improve its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "mi_normalized = scaler.fit_transform(mi.reshape(-1, 1)).flatten()\n",
    "fisher_normalized = scaler.fit_transform(fisher_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Information Gain': mi_normalized,\n",
    "    'Fisher Score': fisher_normalized\n",
    "})\n",
    "\n",
    "# Sort by Information Gain for consistency\n",
    "comparison_df = comparison_df.sort_values(by='Information Gain', ascending=False)\n",
    "\n",
    "# Plotting the comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "\n",
    "# Bar width\n",
    "bar_width = 0.4\n",
    "\n",
    "# Positions of the bars on the x-axis\n",
    "r1 = np.arange(len(comparison_df['Feature']))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "\n",
    "# Make the plot\n",
    "ax.barh(r1, comparison_df['Information Gain'], color='skyblue', height=bar_width, label='Information Gain')\n",
    "ax.barh(r2, comparison_df['Fisher Score'], color='lightgreen', height=bar_width, label='Fisher Score')\n",
    "\n",
    "# Labeling\n",
    "ax.set_xlabel('Normalized Score')\n",
    "ax.set_ylabel('Features')\n",
    "ax.set_title('Comparison of Information Gain and Fisher Score for Tree Phenology Classification')\n",
    "ax.set_yticks([r + bar_width / 2 for r in range(len(comparison_df['Feature']))])\n",
    "ax.set_yticklabels(comparison_df['Feature'])\n",
    "\n",
    "# Invert y-axis\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Removing top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Adding legend\n",
    "ax.legend()\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/comparison_information_gain_fisher_score.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows that the following features : cos_phase_crswir, offset_crswir, and cos_phase_red have high importance in both metrics. Elevation has a low Fisher Score, and high information gain. If we look at the distribution of the elevation across deciduous and evergreen, we can see that the relationship is non linear : deciduous are strongly present in the 250 - 750m segment. The non-linearity of the relationship can explain a low Fisher's score.  \n",
    "\n",
    "Seing the results we can discard the following features : sin_aspect, amplitude green, cos_aspect, sin_phase_blue, amplitude_blue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation \n",
    "Correlation analysis is a statistical method used to evaluate the strength and direction of the linear relationship between two variables. In the context of feature selection for machine learning, understanding the correlation between features and the target variable, as well as among the features themselves, is crucial for several reasons:\n",
    "\n",
    "- Correlation with Target Variable: This helps to identify which features are most strongly associated with the target variable, providing insights into their potential importance in the model.\n",
    "- Correlation Among Features: High correlation among features indicates redundancy, meaning that one feature can be predicted from another. Redundant features do not provide additional information and can lead to overfitting. By identifying these, we can simplify the model by removing or combining correlated features.\n",
    "\n",
    "Importance of Correlation Analysis :\n",
    "- **Identify Important Features:** Features that are highly correlated with the target variable are potential candidates for inclusion in the model.\n",
    "- **Detect Redundancy:** Features that are highly correlated with each other may be redundant. Including redundant features can complicate the model without providing additional predictive power.\n",
    "- **Simplify the Model:** By removing or combining correlated features, we can reduce the complexity of the model, which can help prevent overfitting and improve interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = data[features + [target]].corr()\n",
    "\n",
    "# Define the 'coolwarm' colormap\n",
    "cmap = 'coolwarm'\n",
    "\n",
    "# Plot correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "\n",
    "# Create the main heatmap without a colorbar\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=\".2f\", cmap=cmap, cbar=False, square=True, ax=ax,\n",
    "            vmin=-1, vmax=1)\n",
    "\n",
    "# Define a new axis for the colorbar\n",
    "# cbar_ax = fig.add_axes([0.25, -0.15, 0.4, 0.02])\n",
    "\n",
    "# Create a colorbar\n",
    "# norm = Normalize(vmin=-1, vmax=1)\n",
    "# cbar = ColorbarBase(cbar_ax, cmap=cmap, norm=norm, orientation='horizontal')\n",
    "# cbar.set_ticks([-1, 0, 1])\n",
    "# cbar.set_ticklabels([-1, 0, 1])\n",
    "\n",
    "# Function to color-code the tick labels\n",
    "def color_code_ticks(ax, corr_matrix):\n",
    "    new_labels = []\n",
    "    for label in ax.get_xticklabels():\n",
    "        label_text = label.get_text().replace('_', ' ')\n",
    "        new_labels.append(label_text)\n",
    "        if label_text == target.replace('_', ' '):\n",
    "            label.set_fontweight('bold')\n",
    "        if 'red' in label_text: \n",
    "            label.set_color('red')\n",
    "        elif 'green' in label_text:\n",
    "            label.set_color('green')\n",
    "        elif 'blue' in label_text:\n",
    "            label.set_color('blue')\n",
    "        elif 'crswir' in label_text:\n",
    "            label.set_color('darkred')\n",
    "    ax.set_xticklabels(new_labels, rotation=45, ha='right')\n",
    "\n",
    "    new_labels = []\n",
    "    for label in ax.get_yticklabels():\n",
    "        label_text = label.get_text().replace('_', ' ')\n",
    "        new_labels.append(label_text)\n",
    "        if label_text == target.replace('_', ' '):\n",
    "            label.set_fontweight('bold')\n",
    "        if 'red' in label_text: \n",
    "            label.set_color('red')\n",
    "        elif 'green' in label_text:\n",
    "            label.set_color('green')\n",
    "        elif 'blue' in label_text:\n",
    "            label.set_color('blue')\n",
    "        elif 'crswir' in label_text:\n",
    "            label.set_color('darkred')\n",
    "    ax.set_yticklabels(new_labels, rotation=0)\n",
    "\n",
    "# Apply color coding to the tick labels\n",
    "color_code_ticks(ax, corr_matrix)\n",
    "\n",
    "# Bold the 'phen' row and column in the heatmap annotations\n",
    "for text in ax.texts:\n",
    "    pos = text.get_position()\n",
    "    if pos[0] == list(corr_matrix.columns).index(target) + 0.5 or pos[1] == list(corr_matrix.index).index(target) + 0.5:\n",
    "        text.set_fontweight('bold')\n",
    "\n",
    "ax.set_title('Correlation Matrix of Features and Target')\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/correlation_matrix.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first compare the features for the RGB channels and the CRSWIR. Offsets of the RGB channels are strongly correlated. We can observe the same pattern for amplitude and phase, though with slightly smaller correlations. The CRSWIR features have a small correlation with the RGB ones. Aspect and elevation have little correlation with other features. According to these results, it seems critical to keep the CRSWIR features while retaining only one of the RGB bands for the features (amplitude, offset, and phase).\n",
    "\n",
    "The cos_phase_red, cos_phase_crswir, and offset_crswir are the features with the highest (negative) correlation with tree phenology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suplementary materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most deviated region per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate deviation score for each feature in each region\n",
    "def calculate_deviation_scores(data, features):\n",
    "    regions = data['greco_region'].unique()\n",
    "    deviation_scores = pd.DataFrame(index=regions, columns=features)\n",
    "    \n",
    "    for feature in tqdm(features):\n",
    "        overall_distribution = data[feature]\n",
    "        \n",
    "        for region in regions:\n",
    "            region_data = data[data['greco_region'] == region][feature]\n",
    "            ks_stat, _ = ks_2samp(overall_distribution, region_data)\n",
    "            deviation_scores.loc[region, feature] = ks_stat\n",
    "    \n",
    "    return deviation_scores\n",
    "\n",
    "# Calculate deviation scores\n",
    "deviation_scores = calculate_deviation_scores(data, features)\n",
    "\n",
    "# Identify the region with the highest deviation for each feature\n",
    "#chang dtypes to float\n",
    "deviation_scores = deviation_scores.astype(float)\n",
    "most_deviated_region_per_feature = deviation_scores.idxmax()\n",
    "\n",
    "print(f'Most deviated region per feature:\\n{most_deviated_region_per_feature}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation Inflation Factor (VIF)\n",
    "\n",
    "### What is VIF?\n",
    "\n",
    "Variance Inflation Factor (VIF) is a measure of the amount of multicollinearity in a set of multiple regression variables. It quantifies how much the variance (the square of the standard error) of an estimated regression coefficient is increased because of collinearity. High multicollinearity increases the variance of the coefficient estimates and makes the estimates very sensitive to changes in the model, which can result in less reliable statistical inferences.\n",
    "\n",
    "### How is VIF Computed?\n",
    "\n",
    "1. **For each feature $i$ in the dataset, regress that feature on all the other features.** \n",
    "   \n",
    "   For example, if you have features $X_1, X_2, X_3$, you would perform the following regressions:\n",
    "   - $X_1 \\sim X_2 + X_3$\n",
    "   - $X_2 \\sim X_1 + X_3$\n",
    "   - $X_3 \\sim X_1 + X_2$\n",
    "   \n",
    "2. **Compute the R-squared value ($R^2_i$) from this regression.**\n",
    "\n",
    "   The R-squared value represents the proportion of the variance in the dependent variable that is predictable from the independent variables. In this context, it represents how well feature $i$ can be predicted by the other features.\n",
    "\n",
    "3. **Calculate the VIF for each feature using the formula:**\n",
    "\n",
    "   $$ \\text{VIF}_i = \\frac{1}{1 - R^2_i} $$\n",
    "\n",
    "   Where:\n",
    "   - $R^2_i$ is the R-squared value from regressing the $i$th feature on all the other features.\n",
    "   - VIF values quantify how much the variance of a coefficient is inflated due to multicollinearity.\n",
    "\n",
    "### Interpretation of VIF Values\n",
    "\n",
    "- **VIF = 1**: No correlation between the $i$th feature and the other features. The feature is not collinear.\n",
    "- **1 < VIF < 5**: Moderate correlation that may be acceptable depending on the context.\n",
    "- **VIF ≥ 5**: High correlation and potentially problematic multicollinearity. In some contexts, a threshold of 10 is used instead of 5.\n",
    "\n",
    "### Using VIF\n",
    "\n",
    "1. **Identify multicollinearity**: Calculate VIF for all features to identify those with high VIF values, indicating high multicollinearity.\n",
    "2. **Address multicollinearity**: Consider removing or combining highly collinear features to improve the model's stability and interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from utils import load_and_preprocess_table_data\n",
    "\n",
    "def calculate_vif(data: pd.DataFrame, features: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the Variance Inflation Factor (VIF) for each feature.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset.\n",
    "        features (list[str]): List of feature names to calculate VIF for.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing features and their corresponding VIF values.\n",
    "    \"\"\"\n",
    "    X = data[features]\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['Feature'] = features\n",
    "    vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "\n",
    "config = \"no_resample_cloud_disturbance_weights_3Y\"\n",
    "data = load_and_preprocess_table_data(config)\n",
    "\n",
    "# Define features\n",
    "rgb_features = ['amplitude_red', 'cos_phase_red', 'offset_green', 'offset_red', 'cos_phase_blue']\n",
    "crswir_features = ['amplitude_crswir', 'cos_phase_crswir', 'sin_phase_crswir', 'offset_crswir']\n",
    "relevant_features = rgb_features + crswir_features\n",
    "\n",
    "# Calculate VIF\n",
    "vif_df = calculate_vif(data, relevant_features)\n",
    "print(vif_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_correlation_matrix(data: pd.DataFrame, features: list[str]):\n",
    "    \"\"\"\n",
    "    Plot the correlation matrix for a given list of features.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset.\n",
    "        features (list[str]): List of feature names to include in the correlation matrix.\n",
    "    \"\"\"\n",
    "    correlation_matrix = data[features].corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1)\n",
    "    plt.title('Correlation Matrix of Features')\n",
    "    plt.show()\n",
    "    return correlation_matrix\n",
    "\n",
    "# Plot correlation matrix\n",
    "correlation_matrix = plot_correlation_matrix(data, relevant_features)\n",
    "\n",
    "# Check correlations of `offset_red` and `offset_crswir`\n",
    "print(\"\\nCorrelations with offset_green:\")\n",
    "print(correlation_matrix['offset_green'].sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nCorrelations with offset_crswir:\")\n",
    "print(correlation_matrix['offset_crswir'].sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_pairplot(data: pd.DataFrame, features: list[str]):\n",
    "    \"\"\"\n",
    "    Plot the pair plot for a given list of features.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset.\n",
    "        features (list[str]): List of feature names to include in the pair plot.\n",
    "    \"\"\"\n",
    "    sns.pairplot(data[features])\n",
    "    plt.suptitle('Pair Plot of Relevant Features', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "plot_pairplot(data, relevant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_and_preprocess_table_data\n",
    "\n",
    "def calculate_vif(data: pd.DataFrame, features: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the Variance Inflation Factor (VIF) for each feature.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset.\n",
    "        features (list[str]): List of feature names to calculate VIF for.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing features and their corresponding VIF values.\n",
    "    \"\"\"\n",
    "    X = data[features]\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['Feature'] = features\n",
    "    vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "def plot_pairplot(data: pd.DataFrame, features: list[str]):\n",
    "    \"\"\"\n",
    "    Plot the pair plot for a given list of features.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset.\n",
    "        features (list[str]): List of feature names to include in the pair plot.\n",
    "    \"\"\"\n",
    "    sns.pairplot(data[features])\n",
    "    plt.suptitle('Pair Plot of Relevant Features', y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "# Load data\n",
    "config = \"no_resample_cloud_disturbance_weights_3Y\"\n",
    "data = load_and_preprocess_table_data(config)\n",
    "\n",
    "# Define features\n",
    "red_features = ['amplitude_red', 'cos_phase_red', 'sin_phase_red', 'offset_red']\n",
    "crswir_features = ['amplitude_crswir', 'cos_phase_crswir', 'sin_phase_crswir', 'offset_crswir']\n",
    "relevant_features = red_features + crswir_features\n",
    "\n",
    "# Check for NaNs and Infs in the original data\n",
    "if data[relevant_features].isnull().values.any():\n",
    "    raise ValueError(\"Input data contains NaNs. Please handle them before proceeding.\")\n",
    "if np.isinf(data[relevant_features].values).any():\n",
    "    raise ValueError(\"Input data contains Infs. Please handle them before proceeding.\")\n",
    "\n",
    "# Add a small constant to avoid log(0) and negative values\n",
    "epsilon = 1e-6\n",
    "# Combine `offset_red` and `offset_crswir` using different techniques\n",
    "data['offset_mean'] = data[['offset_red', 'offset_crswir']].mean(axis=1)\n",
    "data['offset_interaction'] = data['offset_red'] * data['offset_crswir']\n",
    "data['offset_log'] = np.log1p(data['offset_red'] + epsilon) + np.log1p(data['offset_crswir'] + epsilon)\n",
    "data['offset_sqrt'] = np.sqrt(data['offset_red'] + epsilon) + np.sqrt(data['offset_crswir'] + epsilon)\n",
    "data['offset_poly'] = (data['offset_red'] + epsilon)**2 + (data['offset_crswir'] + epsilon)**2\n",
    "\n",
    "# Check for NaNs and Infs after transformations\n",
    "combined_features = ['offset_mean', 'offset_interaction', 'offset_log', 'offset_sqrt', 'offset_poly']\n",
    "\n",
    "for feature in combined_features:\n",
    "    if data[feature].isnull().values.any():\n",
    "        print(f\"NaNs detected in {feature}.\")\n",
    "        print(data[['offset_red', 'offset_crswir', feature]].loc[data[feature].isnull()])\n",
    "    if np.isinf(data[feature].values).any():\n",
    "        print(f\"Infs detected in {feature}.\")\n",
    "        print(data[['offset_red', 'offset_crswir', feature]].loc[np.isinf(data[feature].values)])\n",
    "\n",
    "# Drop rows where NaNs are present in the transformed features\n",
    "data = data.dropna(subset=combined_features)\n",
    "\n",
    "# Define new feature sets\n",
    "new_features_mean = [f for f in relevant_features if f not in ['offset_red', 'offset_crswir']] + ['offset_mean']\n",
    "new_features_interaction = [f for f in relevant_features if f not in ['offset_red', 'offset_crswir']] + ['offset_interaction']\n",
    "new_features_log = [f for f in relevant_features if f not in ['offset_red', 'offset_crswir']] + ['offset_log']\n",
    "new_features_sqrt = [f for f in relevant_features if f not in ['offset_red', 'offset_crswir']] + ['offset_sqrt']\n",
    "\n",
    "# Calculate VIF for new feature sets\n",
    "vif_mean = calculate_vif(data, new_features_mean)\n",
    "vif_interaction = calculate_vif(data, new_features_interaction)\n",
    "vif_log = calculate_vif(data, new_features_log)\n",
    "vif_sqrt = calculate_vif(data, new_features_sqrt)\n",
    "\n",
    "print(\"VIF for Mean Combined Feature:\\n\", vif_mean)\n",
    "print(\"VIF for Interaction Combined Feature:\\n\", vif_interaction)\n",
    "print(\"VIF for Logarithmic Combined Feature:\\n\", vif_log)\n",
    "print(\"VIF for Square Root Combined Feature:\\n\", vif_sqrt)\n",
    "\n",
    "# Plot pair plot for the best feature set (for example, using the mean combined feature)\n",
    "plot_pairplot(data, new_features_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between extraction methods and time series length\n",
    "\n",
    "This section compares features extracted from Sentinel-2 time series pixels using various methods over different time spans. By loading and concatenating training and validation datasets, it calculates the mean and standard deviation of pixel-wise differences for amplitude, phase, and offset features. These metrics are stored in a DataFrame and visualized as heatmaps, providing a comprehensive comparison of feature extraction methods across regions. The results help assess the variability and consistency of different extraction methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to compute differences between features\n",
    "def compute_differences(df1: pd.DataFrame, df2: pd.DataFrame, feature: str):\n",
    "    diff = df1[feature] - df2[feature]\n",
    "    mean_diff = diff.mean()\n",
    "    std_diff = diff.std()\n",
    "    return mean_diff, std_diff\n",
    "\n",
    "# Function to plot heatmap\n",
    "def plot_heatmap(data, title, xlabel, ylabel, output_file):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(data, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.savefig(output_file)\n",
    "    plt.close()\n",
    "\n",
    "# Define the configurations and file paths\n",
    "configs = [\"no_resample_cloud_disturbance_weights\", \"no_resample_cloud_weights\", \"no_resample_no_weights\", \"resampled_no_weights\"]\n",
    "years = [1, 2, 3]\n",
    "indices = [\"red\", \"green\", \"blue\", \"crswir\"]\n",
    "features = ['amplitude', 'cos_phase', 'sin_phase', 'offset']\n",
    "all_data = {}\n",
    "\n",
    "# Load and concatenate all data\n",
    "for config in configs:\n",
    "    for year in years:\n",
    "        all_data[f'{config}_Y{year}'] = load_and_preprocess_table_data(f'{config}_{year}Y')\n",
    "\n",
    "# Get unique regions\n",
    "regions = all_data[f'{configs[0]}_Y{1}']['greco_region'].unique()\n",
    "\n",
    "# DataFrame to store all metrics\n",
    "metrics_df = pd.DataFrame(columns=['method', 'year', 'feature', 'index', 'region', 'mean_diff', 'std_diff'])\n",
    "\n",
    "# Compute differences and generate heatmaps\n",
    "for region in regions:\n",
    "    for index in indices:\n",
    "        for feature in features:\n",
    "            # Compare the same method across different years\n",
    "            for config in configs:\n",
    "                for i, year1 in enumerate(years):\n",
    "                    df1 = all_data[f'{config}_Y{year1}']\n",
    "                    df1_region = df1[df1['greco_region'] == region]\n",
    "\n",
    "                    for j, year2 in enumerate(years):\n",
    "                        df2 = all_data[f'{config}_Y{year2}']\n",
    "                        df2_region = df2[df2['greco_region'] == region]\n",
    "                        mean_diff, std_diff = compute_differences(df1_region, df2_region, f'{feature}_{index}')\n",
    "                        \n",
    "                        metrics_df = metrics_df.append({\n",
    "                            'method': config,\n",
    "                            'year': f'{year1}_vs_{year2}',\n",
    "                            'feature': feature,\n",
    "                            'index': index,\n",
    "                            'region': region,\n",
    "                            'mean_diff': mean_diff,\n",
    "                            'std_diff': std_diff\n",
    "                        }, ignore_index=True)\n",
    "\n",
    "            # Compare the same year across different methods\n",
    "            for year in years:\n",
    "                for i, config1 in enumerate(configs):\n",
    "                    df1 = all_data[f'{config1}_Y{year}']\n",
    "                    df1_region = df1[df1['greco_region'] == region]\n",
    "\n",
    "                    for j, config2 in enumerate(configs):\n",
    "                        if config1 == config2:\n",
    "                            continue\n",
    "                        df2 = all_data[f'{config2}_Y{year}']\n",
    "                        df2_region = df2[df2['greco_region'] == region]\n",
    "                        mean_diff, std_diff = compute_differences(df1_region, df2_region, f'{feature}_{index}')\n",
    "                        \n",
    "                        metrics_df = metrics_df.append({\n",
    "                            'method': f'{config1}_vs_{config2}',\n",
    "                            'year': year,\n",
    "                            'feature': feature,\n",
    "                            'index': index,\n",
    "                            'region': region,\n",
    "                            'mean_diff': mean_diff,\n",
    "                            'std_diff': std_diff\n",
    "                        }, ignore_index=True)\n",
    "\n",
    "# Save the metrics DataFrame to a CSV file for further analysis\n",
    "metrics_df.to_csv('metrics_comparison.csv', index=False)\n",
    "\n",
    "# Aggregate and plot heatmaps\n",
    "for index in indices:\n",
    "    for feature in features:\n",
    "        for region in regions:\n",
    "            # Filter the DataFrame for specific feature, index, and region\n",
    "            filtered_df = metrics_df[(metrics_df['feature'] == feature) &\n",
    "                                     (metrics_df['index'] == index) &\n",
    "                                     (metrics_df['region'] == region)]\n",
    "            \n",
    "            # Pivot the DataFrame to get the matrices for heatmap\n",
    "            mean_diff_matrix_years = filtered_df.pivot('year', 'method', 'mean_diff')\n",
    "            std_diff_matrix_years = filtered_df.pivot('year', 'method', 'std_diff')\n",
    "            \n",
    "            mean_diff_matrix_methods = filtered_df.pivot('method', 'year', 'mean_diff')\n",
    "            std_diff_matrix_methods = filtered_df.pivot('method', 'year', 'std_diff')\n",
    "\n",
    "            # Plot the heatmaps for year comparison\n",
    "            plot_heatmap(mean_diff_matrix_years, f\"Mean Difference ({feature} - {index} - {region} - Years)\", \"Year\", \"Method\", f\"mean_diff_{feature}_{index}_{region}_years.png\")\n",
    "            plot_heatmap(std_diff_matrix_years, f\"Std Difference ({feature} - {index} - {region} - Years)\", \"Year\", \"Method\", f\"std_diff_{feature}_{index}_{region}_years.png\")\n",
    "\n",
    "            # Plot the heatmaps for method comparison\n",
    "            plot_heatmap(mean_diff_matrix_methods, f\"Mean Difference ({feature} - {index} - {region} - Methods)\", \"Method\", \"Year\", f\"mean_diff_{feature}_{index}_{region}_methods.png\")\n",
    "            plot_heatmap(std_diff_matrix_methods, f\"Std Difference ({feature} - {index} - {region} - Methods)\", \"Method\", \"Year\", f\"std_diff_{feature}_{index}_{region}_methods.png\")\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Threshold \n",
    "\n",
    "Variance Threshold is a simple feature selection method that removes features with low variance. Features with a variance below a certain threshold do not vary much and hence are less likely to be informative. In other words, features with low variance are less likely to carry significant information that can help differentiate between classes. By removing these low-variance features, we can simplify the model, reduce overfitting, and improve model performance.\n",
    "\n",
    "In this analysis, we will compute the variance of each feature and identify those that have a variance below a specified threshold. We will then visualize these variances to understand which features may be candidates for removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Extract the feature data\n",
    "X = data[features]\n",
    "\n",
    "# Compute variance of each feature\n",
    "variances = X.var()\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "variance_df = pd.DataFrame({'Feature': features, 'Variance': variances})\n",
    "variance_df = variance_df.sort_values(by='Variance', ascending=False)\n",
    "\n",
    "# Plotting the variances\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.barh(variance_df['Feature'], variance_df['Variance'], color='skyblue')\n",
    "ax.set_xlabel('Variance')\n",
    "ax.set_ylabel('Features')\n",
    "ax.set_title('Variance of Features for Tree Phenology Classification')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Removing top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/variance_threshold.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Define variance threshold\n",
    "threshold = 0.5\n",
    "\n",
    "# Apply Variance Threshold\n",
    "selector = VarianceThreshold(threshold=threshold)\n",
    "X_high_variance = selector.fit_transform(X)\n",
    "\n",
    "# Get the support mask\n",
    "support_mask = selector.get_support()\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_features = X.columns[support_mask]\n",
    "\n",
    "print(\"Selected features with variance above threshold:\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.variances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dispersion ratio\n",
    "\n",
    "The Dispersion Ratio is a feature selection method that evaluates the dispersion of each feature. It is based on the concept of the ratio between the variance of a feature and its mean. A high dispersion ratio indicates that the feature values are spread out over a wide range, suggesting that the feature may be informative. Conversely, a low dispersion ratio suggests that the feature values are tightly clustered, potentially indicating redundancy or lack of informativeness.\n",
    "\n",
    "By analyzing the dispersion ratio of each feature, we can identify features that have a wide range of values and are likely to provide useful information for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the feature data\n",
    "X = data[features]\n",
    "\n",
    "# Compute mean and variance of each feature\n",
    "means = X.mean()\n",
    "variances = X.var()\n",
    "\n",
    "# Compute dispersion ratio\n",
    "dispersion_ratio = variances / means\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "dispersion_df = pd.DataFrame({'Feature': features, 'Dispersion Ratio': dispersion_ratio})\n",
    "dispersion_df = dispersion_df.sort_values(by='Dispersion Ratio', ascending=False)\n",
    "\n",
    "# Plotting the dispersion ratios\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.barh(dispersion_df['Feature'], dispersion_df['Dispersion Ratio'], color='skyblue')\n",
    "ax.set_xlabel('Dispersion Ratio')\n",
    "ax.set_ylabel('Features')\n",
    "ax.set_title('Dispersion Ratio of Features for Tree Phenology Classification')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Removing top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/dispersion_ratio.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load training data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Define feature pairs\n",
    "feature_pairs = [\n",
    "    ('amplitude_crswir', 'amplitude_red'),\n",
    "    ('phase_crswir', 'phase_red'),\n",
    "    ('offset_crswir', 'offset_red')\n",
    "]\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_data = train_data[[pair[0] for pair in feature_pairs] + [pair[1] for pair in feature_pairs]]\n",
    "correlation_matrix = correlation_data.corr()\n",
    "\n",
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix between CRSWIR and Red Channel Features')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plots with correlation coefficients\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, (crswir_feature, red_feature) in enumerate(feature_pairs):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.scatterplot(x=train_data[crswir_feature], y=train_data[red_feature])\n",
    "    r = train_data[crswir_feature].corr(train_data[red_feature])\n",
    "    plt.title(f'{crswir_feature} vs {red_feature}\\nCorrelation: {r:.2f}')\n",
    "    plt.xlabel(crswir_feature)\n",
    "    plt.ylabel(red_feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to plot linear regression\n",
    "def plot_linear_regression(data, x_feature, y_feature, ax):\n",
    "    X = data[x_feature].values.reshape(-1, 1)\n",
    "    y = data[y_feature].values\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    ax.scatter(X, y, alpha=0.5)\n",
    "    ax.plot(X, y_pred, color='red', linewidth=2, linestyle='--')\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    x_feature = x_feature.replace('_', ' ').capitalize()\n",
    "    y_feature = y_feature.replace('_', ' ').capitalize()\n",
    "    ax.set_title(f'{x_feature} vs {y_feature}\\nR²: {r2:.2f}')\n",
    "    ax.set_xlabel(x_feature)\n",
    "    ax.set_ylabel(y_feature)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot linear regression\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "for i, (crswir_feature, red_feature) in enumerate(feature_pairs):\n",
    "    ax = plt.subplot(2, 3, i + 1)\n",
    "    plot_linear_regression(train_data, crswir_feature, red_feature, ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('images/linear_regression.png', dpi=300, bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kayrros-default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
