{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection with kmenas = 2 and threshold fixed on the phase_crswir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, adjusted_rand_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils import load_folder, calculate_slope_with_dates\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define the genus mapping\n",
    "\n",
    "phen_mapping = {1: 'deciduous', 2: 'evergreen'}\n",
    "\n",
    "def save_results_as_tif(output_path: str, data: np.ndarray, reference_path: str):\n",
    "    \"\"\"Save clustering results as a .tif file.\"\"\"\n",
    "    rgb_path = os.path.join(reference_path, 'rgb')\n",
    "    rgb_file = [f for f in os.listdir(rgb_path) if f.endswith('.tif')][0]\n",
    "    rgb_path = os.path.join(rgb_path, rgb_file)\n",
    "    with rasterio.open(rgb_path) as src:\n",
    "        meta = src.meta.copy()\n",
    "        meta.update(dtype=rasterio.uint8, count=1)\n",
    "\n",
    "        with rasterio.open(output_path, 'w', **meta) as dst:\n",
    "            dst.write(data.astype(rasterio.uint8), 1)\n",
    "\n",
    "def load_data_from_tile(path: str) -> tuple:\n",
    "    \"\"\"Load and preprocess data from a tile.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the tile directory.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing preprocessed features, weights, forest mask, reference path, and tile shape.\n",
    "    \"\"\"\n",
    "    dates = [datetime.strptime(filename.split('_')[0], '%Y-%m-%d') for filename in os.listdir(os.path.join(path, 'rgb'))]\n",
    "    dates.sort()\n",
    "    rgb = load_folder(os.path.join(path, 'rgb'))\n",
    "    chm = rasterio.open(os.path.join(path, 'tree_map', 'CHM2020.tif')).read(1)\n",
    "    forest_mask = (chm > 250).astype(bool)\n",
    "    slope_map = calculate_slope_with_dates(rgb[:, 0], dates, len(rgb[:, 0]) / 2, len(rgb[:, 0])) / 100\n",
    "    weights = (1 - abs(slope_map.ravel())).clip(0, 1)\n",
    "\n",
    "    path_features = os.path.join(path, 'features')\n",
    "    r_APO = rasterio.open(os.path.join(path_features, 'r_APO.tif')).read()\n",
    "    amplitude_map_r, phase_map_r, offset_map_r = r_APO[0], r_APO[1], r_APO[2]\n",
    "    crswir_APO = rasterio.open(os.path.join(path_features, 'crswir_APO.tif')).read()\n",
    "    amplitude_map_crswir, phase_map_crswir, offset_map_crswir = crswir_APO[0], crswir_APO[1], crswir_APO[2]\n",
    "    rcc_APO = rasterio.open(os.path.join(path_features, 'rcc_APO.tif')).read()\n",
    "    amplitude_map_rcc, phase_map_rcc, offset_map_rcc = rcc_APO[0], rcc_APO[1], rcc_APO[2]\n",
    "    dem = rasterio.open(os.path.join(path_features, 'elevation_aspect.tif')).read()\n",
    "    elevation, aspect = dem[0], dem[1]\n",
    "\n",
    "    features = {\n",
    "        'amplitude_red': amplitude_map_r.ravel(),\n",
    "        'phase_red': phase_map_r.ravel(),\n",
    "        'offset_red': offset_map_r.ravel(),\n",
    "        'amplitude_crswir': amplitude_map_crswir.ravel(),\n",
    "        'phase_crswir': phase_map_crswir.ravel(),\n",
    "        'offset_crswir': offset_map_crswir.ravel(),\n",
    "        'amplitude_rcc': amplitude_map_rcc.ravel(),\n",
    "        'phase_rcc': phase_map_rcc.ravel(),\n",
    "        'offset_rcc': offset_map_rcc.ravel(),\n",
    "        'elevation': elevation.ravel(),\n",
    "        'aspect': aspect.ravel()\n",
    "    }\n",
    "\n",
    "    filtered_features = {k: v[forest_mask.ravel()] for k, v in features.items()}\n",
    "    filtered_weights = weights[forest_mask.ravel()]\n",
    "\n",
    "    # Load reference data\n",
    "    path_reference = os.path.join(path, 'reference_species')\n",
    "    tif = [x for x in os.listdir(path_reference) if x.endswith('.tif')]\n",
    "    reference = rasterio.open(os.path.join(path_reference, tif[0])).read()\n",
    "    genus = reference[1]\n",
    "    phen = reference[2]  # Assuming phenology data is stored in the third band\n",
    "    filtered_genus = genus[forest_mask]\n",
    "    filtered_phen = phen[forest_mask]\n",
    "\n",
    "    filtered_features['genus'] = filtered_genus\n",
    "    filtered_features['phen'] = filtered_phen\n",
    "\n",
    "    return filtered_features, filtered_weights, forest_mask, chm.shape\n",
    "\n",
    "def load_data(directory: str) -> tuple:\n",
    "    \"\"\"Load and preprocess data from all tiles in the directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing tile subdirectories.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing all features, weights, masks, shapes, and paths for each tile.\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    all_weights = []\n",
    "    all_ref = []\n",
    "    tile_shapes = []\n",
    "\n",
    "    for folder in tqdm(os.listdir(directory)):\n",
    "        path = os.path.join(directory, folder)\n",
    "        if folder.endswith('training') or folder.__contains__('.DS_Store') or folder.__contains__('.txt'):\n",
    "            continue\n",
    "        try:\n",
    "            filtered_features, filtered_weights, forest_mask, tile_shape = load_data_from_tile(path)\n",
    "            all_features.append(pd.DataFrame(filtered_features))\n",
    "            all_weights.append(filtered_weights)\n",
    "            all_ref.append(filtered_features['phen'])\n",
    "            tile_shapes.append((path, forest_mask, tile_shape))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {folder}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Stack all features, weights, and references\n",
    "    all_features_stacked = pd.concat(all_features, ignore_index=True)\n",
    "    all_weights_stacked = np.concatenate(all_weights)\n",
    "    all_ref_stacked = np.concatenate(all_ref)\n",
    "\n",
    "    return all_features_stacked, all_weights_stacked, all_ref_stacked, tile_shapes\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "data_dir = '/Users/arthurcalvi/Data/species/validation/tiles'\n",
    "all_features_stacked, all_weights_stacked, all_ref_stacked, tile_shapes = load_data(data_dir)\n",
    "\n",
    "# Preprocess data\n",
    "features = ['amplitude_red', 'phase_red', 'offset_red', 'amplitude_crswir', 'phase_crswir', 'offset_crswir', 'amplitude_rcc', 'phase_rcc', 'offset_rcc', 'elevation', 'aspect']\n",
    "X = all_features_stacked[features]\n",
    "y_phen = all_features_stacked['phen']\n",
    "\n",
    "# Replace infinite values with NaNs\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Handle NaNs by imputing with the mean of each column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Update X with imputed values\n",
    "X = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define feature subset selection using RFE\n",
    "def select_features(X, y, num_features):\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model, n_features_to_select=num_features)\n",
    "    fit = rfe.fit(X, y)\n",
    "    selected_features = [features[i] for i in range(len(features)) if fit.support_[i]]\n",
    "    return selected_features\n",
    "\n",
    "# Classify clusters based on the average phase_crswir value\n",
    "def classify_clusters(X: np.ndarray, clusters: np.ndarray, threshold: float = 0.96) -> np.ndarray:\n",
    "    cluster_labels = np.zeros(clusters.shape)\n",
    "    for cluster in np.unique(clusters):\n",
    "        cluster_mask = clusters == cluster\n",
    "        mean_phase_crswir = np.mean(X[cluster_mask])\n",
    "        cluster_labels[cluster_mask] = 2 if mean_phase_crswir > threshold else 1\n",
    "    return cluster_labels\n",
    "\n",
    "# Placeholder for optimal feature subset and evaluation metric\n",
    "optimal_feature_subset = None\n",
    "max_accuracy = float('-inf')\n",
    "all_clusters = []\n",
    "\n",
    "print(\"Running feature subset selection and clustering...\")\n",
    "# Feature subset selection and clustering\n",
    "list_accuracy_scores = []\n",
    "list_subset_features = []\n",
    "for num_features in tqdm(range(1, len(features) + 1)):\n",
    "    selected_features = select_features(X_scaled, y_phen, num_features)\n",
    "    X_subset = X_scaled[:, [features.index(f) for f in selected_features]]\n",
    "\n",
    "    # Run MiniBatchKMeans clustering\n",
    "    minibatch_kmeans = MiniBatchKMeans(n_clusters=2, random_state=42)\n",
    "    clusters = minibatch_kmeans.fit_predict(X_subset, sample_weight=all_weights_stacked)\n",
    "    all_clusters.append(clusters)\n",
    "\n",
    "    # Classify clusters using the average phase_crswir value\n",
    "    phen_predictions = classify_clusters(X_scaled[:, features.index('phase_crswir')], clusters)\n",
    "\n",
    "    # Evaluate using overall accuracy\n",
    "    valid_mask = (y_phen != 0)\n",
    "    accuracy = accuracy_score(y_phen[valid_mask], phen_predictions[valid_mask])\n",
    "    list_accuracy_scores.append(accuracy)\n",
    "    list_subset_features.append(selected_features)\n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        optimal_feature_subset = selected_features\n",
    "\n",
    "    # Reconstruct cluster maps for each tile\n",
    "    #writing features \n",
    "    print(\"Writing features for each tile...\")\n",
    "    cluster_maps = []\n",
    "    index = 0\n",
    "    for path, forest_mask, tile_shape in tqdm(tile_shapes):\n",
    "        cluster_map = np.full(tile_shape, 0)\n",
    "        n_points = forest_mask.ravel().sum()\n",
    "        cluster_map.ravel()[forest_mask.ravel()] = phen_predictions[index:index + n_points]\n",
    "        index += n_points\n",
    "        cluster_maps.append((path, cluster_map))\n",
    "\n",
    "        # Save results for each tile\n",
    "        result_dir = os.path.join(path, 'results')\n",
    "        os.makedirs(result_dir, exist_ok=True)\n",
    "        output_path = os.path.join(result_dir, f\"{'_'.join(selected_features)}_k2.tif\")\n",
    "        save_results_as_tif(output_path, cluster_map, path)\n",
    "\n",
    "# Compute ARI matrix \n",
    "num_feature_sets = len(all_clusters)\n",
    "ari_matrix = np.zeros((num_feature_sets, num_feature_sets))\n",
    "\n",
    "for i in range(num_feature_sets):\n",
    "    for j in range(num_feature_sets):\n",
    "        ari_matrix[i, j] = adjusted_rand_score(all_clusters[i], all_clusters[j])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Optimal Feature Subset: {optimal_feature_subset}\")\n",
    "print(f\"Maximum Overall Accuracy: {max_accuracy}\")\n",
    "\n",
    "# Visualization of ARI matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(ari_matrix, cmap='RdYlGn', interpolation='nearest')\n",
    "plt.title('ARI Matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Feature Set Index')\n",
    "plt.ylabel('Feature Set Index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualization of Overall Accuracy vs Number of Features\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(range(1, len(features) + 1), list_accuracy_scores, marker='o')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Overall Accuracy')\n",
    "plt.title('Overall Accuracy vs Number of Features')\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Visualization of Feature Importance\n",
    "feature_importance = np.zeros(len(features))\n",
    "for selected_features in list_subset_features:\n",
    "    for feature in selected_features:\n",
    "        feature_importance[features.index(feature)] += 1\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(features, feature_importance)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Selection Frequency')\n",
    "plt.title('Feature Importance (Selection Frequency)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature selection completed and optimal feature subset selected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genus-Aspect space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phen space`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils import load_folder, calculate_slope_with_dates\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define the genus mapping\n",
    "genus_mapping = {\n",
    "    1: 'oak', 2: 'fir', 3: 'pine', 4: 'douglas', 5: 'chestnut',\n",
    "    6: 'spruce', 7: 'larch', 8: 'poplar', 9: 'ash', 10: 'beech',\n",
    "    11: 'alder', 12: 'birch', 13: 'hornbeam', 14: 'locusts'\n",
    "}\n",
    "\n",
    "phen_mapping = {1:'deciduous', 2:'evergreen'} \n",
    "\n",
    "def load_data_from_tile(path: str) -> dict:\n",
    "    \"\"\"Load and preprocess data from a tile.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the tile directory.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing preprocessed features and genus data.\n",
    "    \"\"\"\n",
    "    dates = [datetime.strptime(filename.split('_')[0], '%Y-%m-%d') for filename in os.listdir(os.path.join(path, 'rgb'))]\n",
    "    dates.sort()\n",
    "    rgb = load_folder(os.path.join(path, 'rgb'))\n",
    "    chm = rasterio.open(os.path.join(path, 'tree_map', 'CHM2020.tif')).read(1)\n",
    "    forest_mask = (chm > 250).astype(bool)\n",
    "    slope_map = calculate_slope_with_dates(rgb[:, 0], dates, len(rgb[:, 0]) / 2, len(rgb[:, 0])) / 100\n",
    "    weights = (1 - abs(slope_map.ravel())).clip(0, 1)\n",
    "\n",
    "    path_features = os.path.join(path, 'features')\n",
    "    r_APO = rasterio.open(os.path.join(path_features, 'r_APO.tif')).read()\n",
    "    amplitude_map_r, phase_map_r, offset_map_r = r_APO[0], r_APO[1], r_APO[2]\n",
    "    crswir_APO = rasterio.open(os.path.join(path_features, 'crswir_APO.tif')).read()\n",
    "    amplitude_map_crswir, phase_map_crswir, offset_map_crswir = crswir_APO[0], crswir_APO[1], crswir_APO[2]\n",
    "    rcc_APO = rasterio.open(os.path.join(path_features, 'rcc_APO.tif')).read()\n",
    "    amplitude_map_rcc, phase_map_rcc, offset_map_rcc = rcc_APO[0], rcc_APO[1], rcc_APO[2]\n",
    "    dem = rasterio.open(os.path.join(path_features, 'elevation_aspect.tif')).read()\n",
    "    elevation, aspect = dem[0], dem[1]\n",
    "\n",
    "    features = {\n",
    "        'amplitude_red': amplitude_map_r.ravel(),\n",
    "        'phase_red': phase_map_r.ravel(),\n",
    "        'offset_red': offset_map_r.ravel(),\n",
    "        'amplitude_crswir': amplitude_map_crswir.ravel(),\n",
    "        'phase_crswir': phase_map_crswir.ravel(),\n",
    "        'offset_crswir': offset_map_crswir.ravel(),\n",
    "        'amplitude_rcc': amplitude_map_rcc.ravel(),\n",
    "        'phase_rcc': phase_map_rcc.ravel(),\n",
    "        'offset_rcc': offset_map_rcc.ravel(),\n",
    "        'elevation': elevation.ravel(),\n",
    "        'aspect': aspect.ravel()\n",
    "    }\n",
    "\n",
    "    filtered_features = {k: v[forest_mask.ravel()] for k, v in features.items()}\n",
    "    filtered_weights = weights[forest_mask.ravel()]\n",
    "\n",
    "    # Load reference data\n",
    "    path_reference = os.path.join(path, 'reference_species')\n",
    "    tif = [x for x in os.listdir(path_reference) if x.endswith('.tif')]\n",
    "    reference = rasterio.open(os.path.join(path_reference, tif[0])).read()\n",
    "    genus = reference[1]\n",
    "    phen = reference[2]  # Assuming phenology data is stored in the third band\n",
    "    filtered_genus = genus[forest_mask]\n",
    "    filtered_phen = phen[forest_mask]\n",
    "\n",
    "    filtered_features['genus'] = filtered_genus\n",
    "    filtered_features['phen'] = filtered_phen\n",
    "\n",
    "    return filtered_features, filtered_weights\n",
    "\n",
    "def load_data(directory: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and preprocess data from all tiles in the directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing tile subdirectories.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing all features and genus data.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    all_weights = []\n",
    "\n",
    "    for folder in tqdm(os.listdir(directory)):\n",
    "        path = os.path.join(directory, folder)\n",
    "        if folder.endswith('training') or folder.__contains__('.DS_Store') or folder.__contains__('.txt'):\n",
    "            continue\n",
    "        try:\n",
    "            tile_data, tile_weight = load_data_from_tile(path)\n",
    "            all_data.append(pd.DataFrame(tile_data))\n",
    "            all_weights.append(tile_weight)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {folder}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.concat(all_data, ignore_index=True), np.concatenate(all_weights)\n",
    "\n",
    "# Example usage\n",
    "# Set the directory path where your data is stored\n",
    "print(\"Loading data...\")\n",
    "data_dir = '/Users/arthurcalvi/Data/species/validation/tiles'\n",
    "data, all_weights = load_data(data_dir)\n",
    "\n",
    "# Preprocess data\n",
    "features = ['amplitude_red', 'phase_red', 'offset_red', 'amplitude_crswir', 'phase_crswir', 'offset_crswir', 'amplitude_rcc', 'phase_rcc', 'offset_rcc', 'elevation', 'aspect']\n",
    "X = data[features]\n",
    "y_genus = data['genus']\n",
    "y_phen = data['phen']\n",
    "aspect = data['aspect']\n",
    "genus = data['genus']\n",
    "\n",
    "# Replace infinite values with NaNs\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Handle NaNs by imputing with the mean of each column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Update X with imputed values\n",
    "X = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "# Transform aspect\n",
    "aspect_cos = np.cos(np.radians(aspect))\n",
    "aspect_sin = np.sin(np.radians(aspect))\n",
    "\n",
    "# Add transformed aspect to validation space\n",
    "validation_space = np.stack((y_genus, aspect_cos, aspect_sin), axis=-1)\n",
    "phen_space = y_phen  # Assuming phen is already a suitable space for DBI calculation\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "validation_space_scaled = scaler.fit_transform(validation_space)\n",
    "phen_space_scaled = scaler.fit_transform(phen_space.values.reshape(-1, 1))\n",
    "valid_mask = (y_phen != 0)\n",
    "\n",
    "# Define feature subset selection using RFE\n",
    "def select_features(X, y, num_features):\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model, n_features_to_select=num_features)\n",
    "    fit = rfe.fit(X, y)\n",
    "    selected_features = [features[i] for i in range(len(features)) if fit.support_[i]]\n",
    "    return selected_features\n",
    "\n",
    "# Determine the range for the number of clusters\n",
    "n_genus = len(genus_mapping)\n",
    "cluster_range = range(2, 2 * 8 + 1)\n",
    "\n",
    "# Placeholder for optimal feature subset and DBI\n",
    "optimal_feature_subset = None\n",
    "min_dbi = float('inf')\n",
    "\n",
    "print(\"Running feature subset selection and clustering...\")\n",
    "# Feature subset selection and clustering\n",
    "list_dbi_validation = []\n",
    "list_subset_features = []\n",
    "list_nbr_clusters = []\n",
    "for num_features in tqdm(range(1, len(features) + 1)):\n",
    "    selected_features = select_features(X_scaled[valid_mask], y_phen[valid_mask], num_features)\n",
    "    X_subset = X_scaled[:, [features.index(f) for f in selected_features]]\n",
    "\n",
    "    best_dbi = float('inf')\n",
    "    best_k = None\n",
    "\n",
    "    # Determine optimal number of clusters\n",
    "    # for k in tqdm(cluster_range):\n",
    "    #     minibatch_kmeans = MiniBatchKMeans(n_clusters=k, random_state=42)\n",
    "    #     clusters = minibatch_kmeans.fit_predict(X_subset, sample_weight=all_weights)\n",
    "    #     dbi = davies_bouldin_score(X_subset, clusters)\n",
    "    #     if dbi < best_dbi:\n",
    "    #         best_dbi = dbi\n",
    "    #         best_k = k\n",
    "\n",
    "    best_k = 2\n",
    "\n",
    "    # Run MiniBatchKMeans with the optimal number of clusters\n",
    "    print(f\"Optimal number of clusters: {best_k}\")\n",
    "    print(f\"Optimal number of features: {num_features}\")\n",
    "    print(f\"Optimal features: {selected_features}\")\n",
    "    minibatch_kmeans = MiniBatchKMeans(n_clusters=best_k, random_state=42)\n",
    "    clusters = minibatch_kmeans.fit_predict(X_subset, sample_weight=all_weights)\n",
    "    list_nbr_clusters.append(best_k)\n",
    "\n",
    "    # Compute DBI in the phenology space where phen != 0\n",
    "    dbi_validation = davies_bouldin_score(phen_space_scaled[valid_mask], clusters[valid_mask])\n",
    "    list_dbi_validation.append(dbi_validation)\n",
    "    list_subset_features.append(selected_features)\n",
    "    if dbi_validation < min_dbi:\n",
    "        min_dbi = dbi_validation\n",
    "        optimal_feature_subset = selected_features\n",
    "\n",
    "print(f\"Optimal Feature Subset: {optimal_feature_subset}\")\n",
    "print(f\"Minimum Davies-Bouldin Index in Phenology Space: {min_dbi}\")\n",
    "\n",
    "# Final MiniBatchKMeans clustering with the optimal feature subset\n",
    "X_optimal = X_scaled[:, [features.index(f) for f in optimal_feature_subset]]\n",
    "minibatch_kmeans = MiniBatchKMeans(n_clusters=best_k, random_state=42)\n",
    "clusters = minibatch_kmeans.fit_predict(X_optimal, sample_weight=all_weights)\n",
    "\n",
    "# Visualization of clusters in the Aspect-Genus space\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(aspect_cos, y_phen, c=clusters, cmap='tab20', alpha=0.6)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.xlabel('Aspect (cos)')\n",
    "plt.ylabel('Genus')\n",
    "plt.title('Clusters in the Aspect-Genus Space')\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Visualization of DBI vs Number of Features\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(range(1, len(features) + 1), list_dbi_validation, marker='o')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Davies-Bouldin Index in Phenology Space')\n",
    "plt.title('DBI vs Number of Features')\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Visualization of Feature Importance\n",
    "feature_importance = np.zeros(len(features))\n",
    "for selected_features in list_subset_features:\n",
    "    for feature in selected_features:\n",
    "        feature_importance[features.index(feature)] += 1\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(features, feature_importance)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Selection Frequency')\n",
    "plt.title('Feature Importance (Selection Frequency)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"Clustering completed and optimal feature subset selected.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kayrros-default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
