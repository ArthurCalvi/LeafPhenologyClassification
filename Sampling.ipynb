{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersection with 500 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bbox to validate the algorithm1 and to create the dataset\n",
    "import geopandas as gpd \n",
    "\n",
    "gdf_sampled_bbox = gpd.read_parquet('~/repo/Disturbance-Attribution-Dataset-Joining/data/results/sampling/sampled_bboxes.parquet').to_crs('EPSG:32631')\n",
    "gdf_features = gpd.read_parquet('~/repo/Disturbance-Attribution-Dataset-Joining/data/results/sampling/sampled_features.parquet').to_crs('EPSG:32631')\n",
    "\n",
    "from tqdm import tqdm \n",
    "for i, row in tqdm(enumerate(gdf_sampled_bbox.itertuples())):\n",
    "    bbox = gdf_features[ gdf_features['bbox'] == i ]\n",
    "    start_date = bbox['start_date'].min()\n",
    "    end_date = bbox['end_date'].max()\n",
    "    gdf_sampled_bbox.loc[i, 'start_date'] = start_date\n",
    "    gdf_sampled_bbox.loc[i, 'end_date'] = end_date\n",
    "\n",
    "#add tzinfo to start_date and end_date\n",
    "from datetime import timezone\n",
    "gdf_sampled_bbox.start_date = gdf_sampled_bbox.start_date.dt.tz_localize(timezone.utc) \n",
    "gdf_sampled_bbox.end_date = gdf_sampled_bbox.end_date.dt.tz_localize(timezone.utc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = gpd.read_file('/Users/arthurcalvi/Data/species/france_species.shp').to_crs('EPSG:32631')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_intersection_percentage(sentinel_gdf: gpd.GeoDataFrame, tree_species_gdf: gpd.GeoDataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the percentage of area of Sentinel-2 bounding boxes that contain tree species information for the corresponding years.\n",
    "\n",
    "    Parameters:\n",
    "    sentinel_gdf (gpd.GeoDataFrame): Geodataframe containing Sentinel-2 bounding boxes with start and end dates.\n",
    "    tree_species_gdf (gpd.GeoDataFrame): Geodataframe containing tree species polygons with date information.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A dataframe with Sentinel-2 bounding box IDs, years, and intersection percentages.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    for sentinel_row in sentinel_gdf.itertuples():\n",
    "        # Filter tree species data for the year of the current sentinel row within the date range\n",
    "        start_year = sentinel_row.start_date.year\n",
    "        end_year = sentinel_row.end_date.year\n",
    "        \n",
    "        tree_species_year_gdf = tree_species_gdf[\n",
    "            tree_species_gdf['year'].dt.year.between(start_year, end_year)\n",
    "        ]\n",
    "\n",
    "        # Find intersections\n",
    "        intersections = tree_species_year_gdf[tree_species_year_gdf.intersects(sentinel_row.geometry)]\n",
    "        \n",
    "        # Calculate intersection area\n",
    "        total_intersection_area = intersections.geometry.intersection(sentinel_row.geometry).area.sum()\n",
    "        num_pixels = total_intersection_area / 100  # since 1 pixel = 100 square meters\n",
    "\n",
    "        results.append({\n",
    "            'sentinel_id': sentinel_row.Index,  # Assuming there's an 'id' column in your sentinel data\n",
    "            'year_range': f\"{start_year}-{end_year}\",\n",
    "            'intersection_pixels': num_pixels,\n",
    "            'geometry': intersections.geometry.unary_union\n",
    "        })\n",
    "    \n",
    "    return gpd.GeoDataFrame(results, crs=sentinel_gdf.crs)\n",
    "\n",
    "# Load your geodataframes\n",
    "sentinel_gdf = gdf_sampled_bbox\n",
    "tree_species_gdf = species # Update with your file path\n",
    "\n",
    "# Convert date columns to datetime\n",
    "sentinel_gdf['start_date'] = pd.to_datetime(sentinel_gdf['start_date'])\n",
    "sentinel_gdf['end_date'] = pd.to_datetime(sentinel_gdf['end_date'])\n",
    "tree_species_gdf['year'] = pd.to_datetime(tree_species_gdf['year'])\n",
    "\n",
    "# Compute the intersection percentages\n",
    "intersection_df = calculate_intersection_percentage(sentinel_gdf, tree_species_gdf)\n",
    "\n",
    "# Display the results\n",
    "from IPython.display import display\n",
    "display(intersection_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_df.intersection_pixels.sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select new AOIs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge BDFORET > 2010 and Sarah compilation of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sarah Dataset\n",
    "import geopandas as gpd\n",
    "species = gpd.read_file('/Users/arthurcalvi/Data/species/france_species.shp').to_crs('EPSG:32631')\n",
    "removed_sources = ['FrenchNFI', 'DSF']\n",
    "species = species[~species.source.isin(removed_sources)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BDforet\n",
    "bdforet = gpd.read_parquet('/Users/arthurcalvi/Repo/Disturbance-Attribution-Dataset-Joining/data/processed_datasets/BDFORET_EPSG2154_FR_simplified10.parquet')\n",
    "from thefuzz import process\n",
    "\n",
    "tree_phenology = {\n",
    "    'Pin maritime': 'Evergreen',\n",
    "    'NC': 'Unknown',\n",
    "    'Mixte': 'Mixed',\n",
    "    'Feuillus': 'Deciduous',\n",
    "    'Pins mélangés': 'Mixed',\n",
    "    'NR': 'Unknown',\n",
    "    'Conifères': 'Evergreen',\n",
    "    'Sapin, épicéa': 'Evergreen',\n",
    "    'Peuplier': 'Deciduous',\n",
    "    'Douglas': 'Evergreen',\n",
    "    'Pin sylvestre': 'Evergreen',\n",
    "    'Châtaignier': 'Deciduous',\n",
    "    'Chênes décidus': 'Deciduous',\n",
    "    'Pin laricio, pin noir': 'Evergreen',\n",
    "    'Mélèze': 'Deciduous',\n",
    "    'Pin autre': 'Evergreen',\n",
    "    'Hêtre': 'Deciduous',\n",
    "    'Robinier': 'Deciduous',\n",
    "    'Chênes sempervirents': 'Evergreen',\n",
    "    'Pin d\\'Alep': 'Evergreen',\n",
    "    'Pin à crochets, pin cembro': 'Evergreen',\n",
    "    'Pins mélangés': 'Mixed',\n",
    "    'Sapin, épicéa': 'Evergreen',\n",
    "    'Châtaignier': 'Deciduous',\n",
    "    'Chênes sempervirents': 'Evergreen',\n",
    "    'Pin à crochets, pin cembro': 'Evergreen',\n",
    "    'Hêtre': 'Deciduous',\n",
    "    'Conifères': 'Evergreen',\n",
    "    'Pin maritime': 'Evergreen',\n",
    "    'Mélèze': 'Deciduous',\n",
    "    'Chênes sempervirents': 'Evergreen'\n",
    "}\n",
    "\n",
    "# Function to map species to phenology using fuzzy matching\n",
    "def get_phenology(specie: str) -> str:\n",
    "    match = process.extractOne(specie, tree_phenology.keys(), scorer=process.fuzz.ratio)\n",
    "    if match and match[1] > 50:  # Adjust the threshold as needed\n",
    "        return tree_phenology[match[0]].lower()\n",
    "    else:\n",
    "        print(f'No match found for {specie}')\n",
    "        return 'Unknown'\n",
    "\n",
    "# Create the new phenology column\n",
    "bdforet['phen_en'] = bdforet['ESSENCE'].apply(get_phenology)\n",
    "bdforet = bdforet[(bdforet.phenology != 'unknown') & (bdforet.phenology != 'mixed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the departments of the BDForet created after 2010. \n",
    "import pandas as pd\n",
    "dep = gpd.read_file('/Users/arthurcalvi/Data/Disturbances_maps/BDForet/contour-des-departements.geojson')\n",
    "\n",
    "year = 2010\n",
    "num_dep = ['02',\n",
    " '09',\n",
    " '11',\n",
    " '15',\n",
    " '16',\n",
    " '17',\n",
    " '19',\n",
    " '2A',\n",
    " '2B',\n",
    " '21',\n",
    " '22',\n",
    " '23',\n",
    " '24',\n",
    " '25',\n",
    " '28',\n",
    " '30',\n",
    " '31',\n",
    " '32',\n",
    " '34',\n",
    " '35',\n",
    " '37',\n",
    " '39',\n",
    " '41',\n",
    " '43',\n",
    " '45',\n",
    " '46',\n",
    " '48',\n",
    " '50',\n",
    " '55',\n",
    " '60',\n",
    " '61',\n",
    " '66',\n",
    " '70',\n",
    " '71',\n",
    " '75',\n",
    " '76',\n",
    " '77',\n",
    " '78',\n",
    " '80',\n",
    " '82',\n",
    " '87',\n",
    " '88',\n",
    " '90',\n",
    " '91',\n",
    " '92',\n",
    " '93',\n",
    " '94',\n",
    " '95']\n",
    "dep = dep[ dep['code'].isin(num_dep) ].to_crs(bdforet.crs)\n",
    "dep.geometry = dep.geometry.simplify(1000)\n",
    "\n",
    "bdforet_ = bdforet.clip(dep.geometry)\n",
    "bdforet_.geometry = bdforet_.geometry.buffer(-100)\n",
    "bdforet_cleaned = gpd.overlay(bdforet_, species.to_crs(bdforet_.crs), how='difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add year and dataset \n",
    "bdforet_cleaned['source'] = 'bdforet'\n",
    "bdforet_cleaned['year'] = 2010 \n",
    "\n",
    "bdforet_year = pd.read_csv('/Users/arthurcalvi/Data/Disturbances_maps/BDForet/Année_reference_BDForet.csv', sep=';')\n",
    "\n",
    "# Ensure the same CRS for spatial joins\n",
    "if bdforet_cleaned.crs != dep.crs:\n",
    "    dep = dep.to_crs(bdforet_cleaned.crs)\n",
    "\n",
    "# Step 1: Spatial join bdforet_cleaned with dep to get the region code\n",
    "bdforet_with_code = gpd.sjoin(bdforet_cleaned[['source', 'phen_en', 'geometry']], dep[['code', 'geometry']], how='left', op='intersects')\n",
    "\n",
    "# Step 2: Join bdforet_with_code with bdforet_year to get the year\n",
    "bdforet_with_year = bdforet_with_code.merge(bdforet_year, left_on='code', right_on='N° Dep', how='left')\n",
    "\n",
    "# Step 3: Add the 'year' column from the joined dataframe\n",
    "bdforet_with_year['year'] = bdforet_with_year['Année de référence (PVA)']\n",
    "\n",
    "# Step 4: Drop unnecessary columns and keep only relevant ones\n",
    "bdforet_cleaned_final = bdforet_with_year[['source', 'phen_en', 'geometry', 'year']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write dataset\n",
    "species_ = species[['source', 'phen_en', 'geometry', 'year']].to_crs(bdforet_cleaned_final.crs)\n",
    "merge_dataset = gpd.GeoDataFrame(pd.concat([bdforet_cleaned_final, species_], ignore_index=True), crs=bdforet_cleaned_final.crs)\n",
    "merge_dataset['year'] = merge_dataset['year'].astype(int)\n",
    "merge_dataset.to_parquet('/Users/arthurcalvi/Data/species/dataset_merge_SarahBrood_BDForet-sup2010.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the 2.5km grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_dataset.geometry.iloc[0].is_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and handle problematic geometries\n",
    "for geom in merge_dataset.geometry:\n",
    "    try:\n",
    "        # Try buffering the geometry to fix potential issues\n",
    "        buffered_geom = geom.buffer(0)\n",
    "        valid_geometries.append(buffered_geom)\n",
    "    except Exception as e:\n",
    "        print(f\"Problematic geometry found: {geom.wkt} with error {e}\")\n",
    "        # Optionally log or remove the problematic geometry\n",
    "\n",
    "# Attempt the unary union with the valid geometries\n",
    "try:\n",
    "    boundary_species = unary_union(valid_geometries)\n",
    "except Exception as e:\n",
    "    print(f\"Error during unary union: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box \n",
    "from shapely.ops import unary_union\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# Calculate the different grid sizes\n",
    "initial_grid_size = 100e3\n",
    "aoi_size = 2.5e3\n",
    "\n",
    "sizes = [initial_grid_size]\n",
    "while sizes[-1] > 3*aoi_size:\n",
    "    a = sizes[-1] // 2\n",
    "    sizes.append(max(a, aoi_size))\n",
    "\n",
    "sizes.append(aoi_size)\n",
    "print(sizes)\n",
    "minx, miny, maxx, maxy = merge_dataset.envelope.total_bounds\n",
    "\n",
    "# Step 1: Drop empty geometries (already done)\n",
    "merge_dataset = merge_dataset[~merge_dataset.geometry.is_empty]\n",
    "\n",
    "# Step 2: Ensure all geometries are valid\n",
    "merge_dataset['geometry'] = merge_dataset['geometry'].apply(lambda geom: geom.buffer(0) if not geom.is_valid else geom)\n",
    "\n",
    "# Step 3: Drop null geometries (in case they exist)\n",
    "merge_dataset = merge_dataset[merge_dataset['geometry'].notnull()]\n",
    "\n",
    "# Step 4: Simplify the geometries\n",
    "simplified_geometries = merge_dataset.geometry.simplify(2500, preserve_topology=True)\n",
    "\n",
    "# Step 5: Create unary union\n",
    "try:\n",
    "    boundary_species = simplified_geometries.unary_union\n",
    "except Exception as e:\n",
    "    print(f\"Error during unary union: {e}\")\n",
    "    # Handle exception if necessary\n",
    "    \n",
    "# Create a list to hold selected AOIs\n",
    "new_boundaries = gpd.GeoDataFrame(geometry=[box(minx, miny, maxx, maxy)], crs=merge_dataset.crs)\n",
    "\n",
    "\n",
    "for i, current_grid_size in enumerate(sizes):\n",
    "    print(f\"Processing grid size: {current_grid_size}\")\n",
    "    # Determine the bounds for the current iteration\n",
    "    \n",
    "    # Generate a grid of AOIs\n",
    "    x_coords = np.arange(minx, maxx, current_grid_size)\n",
    "    y_coords = np.arange(miny, maxy, current_grid_size)\n",
    "    \n",
    "    # Create polygons for the AOIs\n",
    "    aoi_polygons = []\n",
    "    for x in x_coords:\n",
    "        for y in y_coords:\n",
    "            aoi = box(x, y, x + current_grid_size, y + current_grid_size)\n",
    "            aoi_polygons.append(aoi)\n",
    "    \n",
    "    # Create a GeoDataFrame from the AOIs\n",
    "    aois = gpd.GeoDataFrame(geometry=aoi_polygons, crs=merge_dataset.crs)\n",
    "   \n",
    "    # Calculate the intersection of each AOI with the species dataset\n",
    "    selected_aois = []\n",
    "    bounds = new_boundaries.unary_union\n",
    "    for row in tqdm(aois.itertuples()):\n",
    "        if row.geometry.intersects(bounds) and row.geometry.intersects(boundary_species):\n",
    "            selected_aois.append(row.geometry)\n",
    "    \n",
    "    print(F'Conversion : {len(selected_aois)/aois.shape[0] :.0%}')\n",
    "    \n",
    "    # Update new_boundaries for the next iteration\n",
    "     \n",
    "    new_boundaries = gpd.GeoDataFrame(geometry=selected_aois, crs=merge_dataset.crs)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    new_boundaries.plot(ax=ax, edgecolor='k')\n",
    "    ax.set_axis_off()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = []\n",
    "geom = []\n",
    "for row in tqdm(new_boundaries.itertuples()):\n",
    "    intersection = boundary_species.intersection(row.geometry)\n",
    "    intersection_area = intersection.area\n",
    "    aoi_area = aoi.area\n",
    "    intersection_percent = (intersection_area / aoi_area) * 100\n",
    "    perc.append(intersection_percent)\n",
    "    geom.append(row.geometry)\n",
    "\n",
    "gdf_intersection = gpd.GeoDataFrame(data=perc, geometry=geom, columns=['perc'], crs=species.crs)\n",
    "gdf_intersection.to_parquet('/Users/arthurcalvi/Data/species/2c5km_cell_percentage.parquet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "gdf_intersection = gpd.read_parquet('/Users/arthurcalvi/Data/species/2c5km_cell_percentage.parquet').to_crs('epsg:2154')\n",
    "gdf_intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greco = gpd.read_file('/Users/arthurcalvi/Data/eco-regions/France/ser_l93_new/ser_l93_new.dbf')\n",
    "greco['greco'] = greco.codeser.apply(lambda x:x[0])\n",
    "greco = greco.dissolve(by='greco', aggfunc='first')\n",
    "greco = greco.reset_index().iloc[1:].to_crs('EPSG:2154')\n",
    "greco.plot(column='codeser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_intersection_2154 = gdf_intersection.to_crs('EPSG:2154')\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "greco.plot(ax=ax, column='codeser', edgecolor='black')\n",
    "gdf_intersection_2154.plot(column='perc', ax=ax, legend=True, legend_kwds={'label': \"Intersection percentage\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sample_gdf_intersection(gdf_intersection: gpd.GeoDataFrame, greco: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Sample from gdf_intersection based on greco regions, selecting the top 32 rows with the highest 'perc'.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "\n",
    "    # Ensure both GeoDataFrames have the same CRS\n",
    "    gdf_intersection.to_crs('epsg:2154', inplace=True)\n",
    "    greco.to_crs('epsg:2154', inplace=True)\n",
    "\n",
    "    for region in tqdm(greco.itertuples(), total=len(greco)):\n",
    "        # Find intersections with the current greco region\n",
    "        intersecting = gdf_intersection[gdf_intersection.intersects(region.geometry)]\n",
    "        \n",
    "        if not intersecting.empty:\n",
    "            # Sort by 'perc' and keep the 32 highest rows\n",
    "            intersecting_sorted = intersecting.sort_values(by='perc', ascending=False)\n",
    "            \n",
    "            # Assign the region name to the intersecting data\n",
    "            intersecting_sorted['NomSER'] = region.NomSER\n",
    "            \n",
    "            # Append the intersecting_sorted to the result\n",
    "            result.append(intersecting_sorted)\n",
    "    \n",
    "    # Combine all results into a single GeoDataFrame\n",
    "    result_gdf = gpd.GeoDataFrame(pd.concat(result, ignore_index=True), crs=gdf_intersection.crs)\n",
    "    return result_gdf[['perc', 'NomSER', 'geometry']]\n",
    "\n",
    "result_gdf_all = sample_gdf_intersection(gdf_intersection, greco)\n",
    "\n",
    "\n",
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "# from shapely.geometry import Polygon\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def sample_gdf_intersection(gdf_intersection: gpd.GeoDataFrame, greco: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "#     \"\"\"\n",
    "#     Sample from gdf_intersection based on greco regions, create training and validation sets.\n",
    "#     \"\"\"\n",
    "#     result = []\n",
    "\n",
    "#     for region in tqdm(greco.itertuples()):\n",
    "#         # Clip gdf_intersection with the current greco region\n",
    "#         clipped = gpd.clip(gdf_intersection, region.geometry)\n",
    "        \n",
    "#         if not clipped.empty:\n",
    "#             # Sort by 'perc' and keep the 8 highest rows\n",
    "#             clipped_sorted = clipped.sort_values(by='perc', ascending=False).head(32)\n",
    "            \n",
    "#             # Randomly split into training and validation sets\n",
    "#             clipped_sorted['set'] = np.where(np.random.rand(len(clipped_sorted)) < 0.5, 'training', 'validation')\n",
    "            \n",
    "#             # Ensure 4 for training and 4 for validation\n",
    "#             training_set = clipped_sorted[clipped_sorted['set'] == 'training'].head(16)\n",
    "#             validation_set = clipped_sorted[clipped_sorted['set'] == 'validation'].head(16)\n",
    "            \n",
    "#             if len(training_set) < 4:\n",
    "#                 additional_training = clipped_sorted[clipped_sorted['set'] == 'validation'].tail(4 - len(training_set))\n",
    "#                 training_set = pd.concat([training_set, additional_training])\n",
    "            \n",
    "#             if len(validation_set) < 4:\n",
    "#                 additional_validation = clipped_sorted[clipped_sorted['set'] == 'training'].tail(4 - len(validation_set))\n",
    "#                 validation_set = pd.concat([validation_set, additional_validation])\n",
    "            \n",
    "#             # Append the training and validation sets to the result\n",
    "#             for set_type, data in zip(['training', 'validation'], [training_set, validation_set]):\n",
    "#                 data['set'] = set_type\n",
    "#                 data['NomSER'] = region.NomSER\n",
    "#                 result.append(data)\n",
    "    \n",
    "#     # Combine all results into a single GeoDataFrame\n",
    "#     result_gdf = gpd.GeoDataFrame(pd.concat(result, ignore_index=True), crs=gdf_intersection.crs)\n",
    "#     return result_gdf[['perc', 'set', 'NomSER', 'geometry']]\n",
    "\n",
    "# result_gdf = sample_gdf_intersection(gdf_intersection, greco)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gdf_all.to_parquet(\"/Users/arthurcalvi/Data/species/validation/2c5km_val_train_tiles_all.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "result_gdf = gpd.read_parquet(\"/Users/arthurcalvi/Data/species/validation/val_train_tiles.parquet\")\n",
    "print(result_gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import geopandas as gpd\n",
    "import os \n",
    "\n",
    "# Sample data for gdf4\n",
    "# gdf4 = gpd.read_file(\"path_to_your_shapefile.shp\")\n",
    "#same crs\n",
    "greco = greco.to_crs('epsg:2154')\n",
    "result_gdf = result_gdf.to_crs('epsg:2154')\n",
    "# Create the figure and axis\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "\n",
    "# Define the colormap\n",
    "# colormap = {'validation': 'yellow', 'training': 'blue'}\n",
    "# result_gdf['color'] = result_gdf['set'].map(colormap)\n",
    "\n",
    "greco.plot(ax=ax, column='NomSER', edgecolor='k', alpha=0.25, cmap='tab20')\n",
    "# Plot the GeoDataFrame with the specified colors\n",
    "#get fraction of a cmap to set the color\n",
    "result_gdf.plot(ax=ax, cmap='viridis', column='perc', legend=True, legend_kwds={'label': \"Intersection percentage\", 'shrink': 0.5, 'orientation': 'horizontal'})\n",
    "\n",
    "# Add basemap and remove axis\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Add a legend\n",
    "# handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colormap[key], markersize=10, label=key) for key in colormap]\n",
    "# ax.legend(handles=handles, title='Set', loc='upper right')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "os.makedirs('images', exist_ok=True)\n",
    "fig.savefig('images/validation_data.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify result_gdf for chronos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_value_maps(species: gpd.GeoDataFrame) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Creates a consistent mapping from unique categorical values to integers for all specified columns.\n",
    "    \"\"\"\n",
    "    columns = ['specie_en', 'genus_en', 'phen_en', 'year', 'source']\n",
    "    value_maps = {}\n",
    "    \n",
    "    for column in columns:\n",
    "        unique_values = species[column].unique()\n",
    "        value_map = {val: idx + 1 for idx, val in enumerate(unique_values)}\n",
    "        value_maps[column] = value_map\n",
    "    \n",
    "    return value_maps\n",
    "\n",
    "def calculate_percentages(clipped: gpd.GeoDataFrame, value_maps: Dict[str, Dict[str, int]]) -> Tuple[int, float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculates the most frequent year and the percentages of deciduous and evergreen types.\n",
    "    \"\"\"\n",
    "    if clipped.empty:\n",
    "        return np.nan, 0.0, 0.0, 0.0\n",
    "    \n",
    "    # Year calculations\n",
    "    year_counts = clipped['year'].map(value_maps['year']).value_counts()\n",
    "    most_frequent_year = year_counts.idxmax()\n",
    "    perc_year = year_counts.max() / year_counts.sum()\n",
    "    \n",
    "    # Phenology calculations\n",
    "    phen_counts = clipped['phen_en'].map(value_maps['phen_en']).value_counts()\n",
    "    deciduous_count = phen_counts.get(value_maps['phen_en'].get('deciduous', 0), 0)\n",
    "    evergreen_count = phen_counts.get(value_maps['phen_en'].get('evergreen', 0), 0)\n",
    "    total_count = phen_counts.sum()\n",
    "    \n",
    "    perc_deciduous = deciduous_count / total_count\n",
    "    perc_evergreen = evergreen_count / total_count\n",
    "    \n",
    "    return most_frequent_year, perc_year, perc_deciduous, perc_evergreen\n",
    "\n",
    "def main(gdf: gpd.GeoDataFrame, species: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Main function to process all tiles, clip species data, and update the GeoDataFrame with calculated percentages.\n",
    "    \"\"\"\n",
    "    # Create value maps for categorical columns\n",
    "    value_maps = create_value_maps(species)\n",
    "\n",
    "    # Initialize lists to hold new column data\n",
    "    years = []\n",
    "    perc_years = []\n",
    "    perc_deciduous = []\n",
    "    perc_evergreen = []\n",
    "\n",
    "    for tile_row in tqdm(gdf.itertuples()):\n",
    "        # Convert the tile to the same CRS as the species data\n",
    "        tile = gpd.GeoDataFrame(geometry=[tile_row.geometry], crs=gdf.crs).to_crs(species.crs)\n",
    "        \n",
    "        # Clip species data with the tile\n",
    "        clipped = gpd.clip(species, tile.geometry)\n",
    "        \n",
    "        # Calculate the necessary percentages and year information\n",
    "        most_frequent_year, perc_year, perc_deciduous_val, perc_evergreen_val = calculate_percentages(clipped, value_maps)\n",
    "        \n",
    "        # Append results to lists\n",
    "        years.append(most_frequent_year)\n",
    "        perc_years.append(perc_year)\n",
    "        perc_deciduous.append(perc_deciduous_val)\n",
    "        perc_evergreen.append(perc_evergreen_val)\n",
    "\n",
    "    # Update the GeoDataFrame with the new columns\n",
    "    gdf['year'] = years\n",
    "    gdf['perc_year'] = perc_years\n",
    "    gdf['perc_deciduous'] = perc_deciduous\n",
    "    gdf['perc_evergreen'] = perc_evergreen\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# Example usage\n",
    " # Replace with actual species data file path\n",
    "# result_gdf = gpd.read_parquet(\"/Users/arthurcalvi/Data/species/validation/2c5km_val_train_tiles.parquet\")\n",
    "\n",
    "# updated_gdf = main(result_gdf.to_crs(species.crs), species)\n",
    "# updated_gdf.to_crs('epsg:2154').to_parquet(\"/Users/arthurcalvi/Data/species/validation/val_train_tiles_2_5_km.parquet\")\n",
    "\n",
    "\n",
    "updated_gdf_all = main(result_gdf_all.to_crs(species.crs), species)\n",
    "updated_gdf_all.to_crs('epsg:2154').to_parquet(\"/Users/arthurcalvi/Data/species/validation/val_train_tiles_2_5_km_all.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_real_greco = {'Côtes_et_plateaux_de_la_Manche': 'Centre Nord semi-océanique',\n",
    "                      'Côtes_et_plateaux_de_la_Manche': 'Centre Nord semi-océanique',\n",
    "                      'Ardenne_primaire': 'Grand Est semi-continental',\n",
    "                      'Préalpes_du_Nord': 'Alpes',\n",
    "                      'Préalpes_du_Nord': 'Alpes',\n",
    "                      'Garrigues' : 'Méditerranée',\n",
    "                      'Massif_vosgien_central': 'Vosges',\n",
    "                        'Premier_plateau_du_Jura': 'Jura',\n",
    "                        'Piémont_pyrénéen' : 'Pyrénées',\n",
    "                        'Terres_rouges': 'Sud-Ouest océanique' ,\n",
    "                          'Corse_occidentale': 'Corse',\n",
    "                        \"Châtaigneraie_du_Centre_et_de_l'Ouest\": 'Massif central' ,\n",
    "                        'Ouest-Bretagne_et_Nord-Cotentin': 'Grand Ouest cristallin et océanique', \n",
    "                        'Total': 'Total'}\n",
    "\n",
    "mapping_real_greco = {k.replace('_', ' '): v for k, v in mapping_real_greco.items()}\n",
    "\n",
    "# updated_gdf['NomSER'] = updated_gdf['NomSER'].map(mapping_real_greco)\n",
    "# updated_gdf.to_parquet(\"/Users/arthurcalvi/Data/species/validation/val_train_tiles_2_5_km.parquet\")\n",
    "updated_gdf_all['NomSER'] = updated_gdf_all['NomSER'].map(mapping_real_greco)\n",
    "updated_gdf_all.to_parquet(\"/Users/arthurcalvi/Data/species/validation/val_train_tiles_2_5_km_all.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load the result_gdf5 with 5km wide tiles\n",
    "result_gdf5 = gpd.read_parquet(\"/Users/arthurcalvi/Data/species/validation/val_train_tiles.parquet\")\n",
    "\n",
    "# Load the result_gdf with 2.5km wide tiles\n",
    "result_gdf = gpd.read_parquet(\"/Users/arthurcalvi/Data/species/validation/val_train_tiles_2_5_km_all.parquet\")\n",
    "\n",
    "def select_non_intersecting_tiles(gdf2_5: gpd.GeoDataFrame, gdf5: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Select one tile for each unique value of the column 'NomSER' from gdf2_5 that does not intersect any tile in gdf5\n",
    "    and has 'deciduous' and 'evergreen' values greater than 0.1.\n",
    "\n",
    "    Args:\n",
    "    gdf2_5 (gpd.GeoDataFrame): GeoDataFrame with 2.5 km wide tiles.\n",
    "    gdf5 (gpd.GeoDataFrame): GeoDataFrame with 5 km wide tiles.\n",
    "\n",
    "    Returns:\n",
    "    gpd.GeoDataFrame: Selected non-intersecting tiles from gdf2_5.\n",
    "    \"\"\"\n",
    "    selected_indices = []\n",
    "\n",
    "    unique_values = gdf2_5['NomSER'].unique()\n",
    "    \n",
    "    for value in unique_values:\n",
    "        tiles = gdf2_5[gdf2_5['NomSER'] == value]\n",
    "        for idx, tile in tiles.iterrows():\n",
    "            if (tile['perc_deciduous'] > 0.25 and tile['perc_evergreen'] > 0.25 and \n",
    "                not gdf5.intersects(tile.geometry).any()):\n",
    "                selected_indices.append(idx)\n",
    "                break\n",
    "    \n",
    "    return selected_indices\n",
    "\n",
    "# Ensure both GeoDataFrames have the same CRS\n",
    "result_gdf5 = result_gdf5.to_crs(epsg=2154)\n",
    "result_gdf = result_gdf.to_crs(epsg=2154)\n",
    "\n",
    "selected_indices = select_non_intersecting_tiles(result_gdf, result_gdf5)\n",
    "\n",
    "# Add 'set' column with default value None\n",
    "result_gdf['set'] = None\n",
    "\n",
    "# Update the 'set' column for the selected tiles\n",
    "result_gdf.loc[selected_indices, 'set'] = 'validation'\n",
    "\n",
    "# Save the updated result_gdf\n",
    "result_gdf.to_parquet(\"/Users/arthurcalvi/Data/species/validation/val_train_tiles_2_5_km.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gdf[result_gdf['set'] == 'validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_value_maps(species: gpd.GeoDataFrame) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Creates a consistent mapping from unique categorical values to integers for all specified columns.\n",
    "    \"\"\"\n",
    "    columns = ['specie_en', 'genus_en', 'phen_en', 'year', 'source']\n",
    "    value_maps = {}\n",
    "    \n",
    "    for column in columns:\n",
    "        unique_values = species[column].unique()\n",
    "        value_map = {val: idx + 1 for idx, val in enumerate(unique_values)}\n",
    "        value_maps[column] = value_map\n",
    "    \n",
    "    return value_maps\n",
    "\n",
    "def process_tile(tile: gpd.GeoDataFrame, species: gpd.GeoDataFrame, value_maps: Dict[str, Dict[str, int]], transform: rasterio.Affine, shape: Tuple[int, int], crs: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Clips species data with the tile and rasterizes specified columns using provided value maps.\n",
    "    \"\"\"\n",
    "    species = species.to_crs(crs)\n",
    "    clipped = gpd.clip(species, tile.geometry)\n",
    "    if clipped.empty:\n",
    "        return np.zeros((len(value_maps), *shape), dtype=np.int32)\n",
    "    \n",
    "    rasters = np.zeros((len(value_maps), *shape), dtype=np.int32)\n",
    "    for i, (column, value_map) in enumerate(value_maps.items()):\n",
    "        shapes = ((geom, value_map[val]) for geom, val in zip(clipped.geometry, clipped[column]))\n",
    "        rasters[i] = rasterize(\n",
    "            shapes,\n",
    "            out_shape=shape,\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            all_touched=True,\n",
    "            dtype=np.int32\n",
    "        )\n",
    "    \n",
    "    return rasters\n",
    "\n",
    "def main(gdf: gpd.GeoDataFrame, species: gpd.GeoDataFrame, output_dir: str, resolution: int = 10, name: str = 'tiles_2_5_km'):\n",
    "    \"\"\"\n",
    "    Main function to process all tiles, clip species data, rasterize and save the results.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create value maps for categorical columns\n",
    "    value_maps = create_value_maps(species)\n",
    "\n",
    "    for index, tile_row in tqdm(gdf.iterrows(), total=gdf.shape[0]):\n",
    "        # s = tile_row['set']\n",
    "        ser = tile_row['NomSER'].replace(' ', '_')\n",
    "        perc = tile_row['perc']\n",
    "        \n",
    "        # Find folder inside subfolder tiles that has name which starts with tile_{index}\n",
    "        folder_match = [f for f in os.listdir(os.path.join(output_dir, name)) if f.startswith(f\"tile_{index}\")]\n",
    "        # Sort folder according to the tile numerous\n",
    "        folder_match = sorted(folder_match, key=lambda x: int(x.split('_')[1]))\n",
    "        \n",
    "        if len(folder_match) > 0:\n",
    "            folder_match = folder_match[0]\n",
    "            rgb_folder = os.path.join(output_dir, name, folder_match, 'rgb')\n",
    "            if os.path.isdir(rgb_folder):\n",
    "                # Get the first raster file in the 'rgb' folder\n",
    "                raster_files = [f for f in os.listdir(rgb_folder) if f.endswith('.tif')]\n",
    "                if raster_files:\n",
    "                    with rasterio.open(os.path.join(rgb_folder, raster_files[0])) as src:\n",
    "                        transform = src.transform\n",
    "                        shape = src.shape\n",
    "                        crs = src.crs\n",
    "\n",
    "                        # Convert the tile to the same CRS as the raster\n",
    "                        tile = gpd.GeoDataFrame(geometry=[tile_row.geometry], crs=gdf.crs).to_crs(crs)\n",
    "                        \n",
    "                        rasters = process_tile(tile, species, value_maps, transform, shape, crs)\n",
    "                        \n",
    "                        out_meta = src.meta.copy()\n",
    "                        out_meta.update({\n",
    "                            'count': rasters.shape[0],\n",
    "                            'dtype': 'int32'\n",
    "                        })\n",
    "\n",
    "                        output_folder = os.path.join(output_dir, name, folder_match, 'reference_species')\n",
    "                        os.makedirs(output_folder, exist_ok=True)\n",
    "                        output_path = os.path.join(output_folder, f\"tile_{index}_{ser}_{perc :.0f}.tif\")\n",
    "                        with rasterio.open(output_path, 'w', **out_meta) as dest:\n",
    "                            dest.write(rasters)\n",
    "\n",
    "    # Write the mapping information to a single text file\n",
    "    txt_output_path = f\"{output_dir}/tiles/value_mappings.txt\"\n",
    "    with open(txt_output_path, 'w') as f:\n",
    "        for column, value_map in value_maps.items():\n",
    "            f.write(f\"Column '{column}' index mapping:\\n\")\n",
    "            for value, idx in value_map.items():\n",
    "                f.write(f\"  {idx}: {value}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "output_dir = \"/Users/arthurcalvi/Data/species/validation\"\n",
    "\n",
    "main(result_gdf, species, output_dir, name='tiles_2_5_km')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure and table for article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = [51, 61, 36, 52, 59, 62, 63, 69, 70, 77]\n",
    "#exclude those index from the result_gdf\n",
    "\n",
    "safe_result_gdf = result_gdf[~result_gdf.index.isin(exclude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import geopandas as gpd\n",
    "import os \n",
    "\n",
    "# Sample data for gdf4\n",
    "# gdf4 = gpd.read_file(\"path_to_your_shapefile.shp\")\n",
    "\n",
    "# Create the figure and axis\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "\n",
    "# Define the colormap\n",
    "colormap = {'validation': 'yellow', 'training': 'blue'}\n",
    "# result_gdf['color'] = result_gdf['set'].map(colormap)\n",
    "\n",
    "greco.plot(ax=ax, column='NomSER', edgecolor='k', alpha=0.25, cmap='tab20', legend=True)\n",
    "# Plot the GeoDataFrame with the specified colors\n",
    "safe_result_gdf.to_crs(greco.crs).plot(ax=ax, color=safe_result_gdf['color'])\n",
    "\n",
    "# Add basemap and remove axis\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Add a legend\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colormap[key], markersize=10, label=key) for key in colormap]\n",
    "# ax.legend(handles=handles, title='Set', loc='upper right')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# os.makedirs('images', exist_ok=True)\n",
    "# fig.savefig('images/validation_data.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_total_pixels(perc: float, resolution: int = 10, tile_size: int = 5000) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the total number of pixels based on percentage of coverage and tile size.\n",
    "    \"\"\"\n",
    "    total_area = tile_size * tile_size\n",
    "    total_pixels = total_area * (perc / 100)\n",
    "    pixel_count = total_pixels / (resolution * resolution)\n",
    "    return int(pixel_count)\n",
    "\n",
    "def intersect_and_summarize(gdf: gpd.GeoDataFrame, species: gpd.GeoDataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform intersection and summarize counts of phenology and total pixel counts per NomSER and set.\n",
    "    \"\"\"\n",
    "    gdf = gdf.to_crs(species.crs)\n",
    "    results = []\n",
    "\n",
    "    for tile in tqdm(gdf.itertuples(), total=len(gdf)):\n",
    "        tile_set = tile.set\n",
    "        tile_perc = tile.perc\n",
    "        tile_nomser = tile.NomSER\n",
    "        tile_geom = tile.geometry\n",
    "\n",
    "        # Perform intersection\n",
    "        intersected = gpd.overlay(species, gpd.GeoDataFrame(geometry=[tile_geom], crs=species.crs), how='intersection')\n",
    "        if not intersected.empty:\n",
    "            intersected['area'] = intersected.area  # Calculate area of intersection\n",
    "            phen_area = intersected.groupby('phen_en')['area'].sum()\n",
    "            phen_total_pixels = phen_area / 100  # since 1 pixel = 100 square meters \n",
    "\n",
    "            for phen, pixels in phen_total_pixels.items():\n",
    "                results.append({\n",
    "                    'set': tile_set,\n",
    "                    'NomSER': tile_nomser,\n",
    "                    'phen_en': phen,\n",
    "                    'total_pixels': pixels\n",
    "                })\n",
    "        else:\n",
    "            results.append({\n",
    "                'set': tile_set,\n",
    "                'NomSER': tile_nomser,\n",
    "                'phen_en': 'None',\n",
    "                'total_pixels': 0\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def summarize_to_table(summary_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarize the intersection results into a table.\n",
    "    \"\"\"\n",
    "    table = summary_df.groupby(['NomSER', 'set', 'phen_en']).agg({\n",
    "        'total_pixels': 'sum'\n",
    "    }).reset_index()\n",
    "    return table\n",
    "\n",
    "def main(gdf, species):\n",
    "    # Example usage\n",
    "    summary_df = intersect_and_summarize(gdf, species)\n",
    "    summary_table = summarize_to_table(summary_df)\n",
    "    \n",
    "    # Save or display the summary table\n",
    "    summary_table.to_csv(\"summary_table.csv\", index=False)\n",
    "\n",
    "    return summary_table\n",
    "\n",
    "# Assuming gdf4 and species are already defined\n",
    "summary_table = main(safe_result_gdf, species)\n",
    "print(summary_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
