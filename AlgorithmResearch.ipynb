{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook Description:**\n",
    "\n",
    "This notebook is designed to test and optimize a series of machine learning algorithms for classifying pixels as deciduous or evergreen. The selected algorithms range from simple to complex, ensuring a comprehensive evaluation of their performance. The notebook is structured to include data preprocessing, model training with hyperparameter optimization, and evaluation of various metrics. Additionally, the notebook includes a checkpointing mechanism, allowing the process to be stopped and resumed, which is particularly useful given limited computational resources.\n",
    "\n",
    "**Steps in the Notebook:**\n",
    "\n",
    "1. **Data Loading and Preprocessing:**\n",
    "   - Data is loaded using a predefined function.\n",
    "   - Features and target variables are defined.\n",
    "   - Standardization is applied selectively based on algorithm requirements.\n",
    "\n",
    "2. **Model Definition:**\n",
    "   - A set of machine learning algorithms is defined, including Logistic Regression, Linear Discriminant Analysis, Quadratic Discriminant Analysis, K-Nearest Neighbors, Support Vector Classifier, Random Forest, XGBoost, and MLP Classifier.\n",
    "\n",
    "3. **Parameter Grids:**\n",
    "   - Parameter grids for each algorithm are specified, with values adjusted to balance resource constraints and optimization needs.\n",
    "\n",
    "4. **Checkpointing Mechanism:**\n",
    "   - Intermediate results and the current state of hyperparameter search are saved to enable stopping and resuming the process.\n",
    "   - Functions `save_checkpoint` and `load_checkpoint` are implemented to handle checkpointing.\n",
    "\n",
    "5. **Model Training and Evaluation:**\n",
    "   - HalvingGridSearchCV is used for efficient hyperparameter optimization.\n",
    "   - Models are evaluated using stratified k-fold cross-validation.\n",
    "   - Evaluation metrics include accuracy, precision, recall, and F1-score, with F1-score used for ranking.\n",
    "\n",
    "6. **Regional Metrics Breakdown:**\n",
    "   - Evaluation metrics are further broken down by regions using the 'greco_region' column, providing insights into geographical variations in model performance.\n",
    "\n",
    "7. **Results Display:**\n",
    "   - The final evaluation results and regional metrics breakdown are displayed for easy interpretation.\n",
    "\n",
    "This structured approach ensures a thorough comparison of models while maintaining flexibility and efficiency in handling computational constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from utils import load_and_preprocess_table_data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier, BernoulliRBM\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV, StratifiedGroupKFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "Y = 3\n",
    "config = f\"no_resample_cloud_disturbance_weights_{Y}Y\"\n",
    "CV = 'Group'\n",
    "extra = config + '_' + CV\n",
    "data = load_and_preprocess_table_data(config)\n",
    "# Define features and target\n",
    "features = ['amplitude_red', 'cos_phase_red','offset_red',\n",
    "            'cos_phase_blue', \n",
    "            'amplitude_crswir', 'cos_phase_crswir', 'sin_phase_crswir', 'offset_crswir', \n",
    "            'elevation']\n",
    "\n",
    "target = 'phen'\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "# Adjust target labels to start from 0\n",
    "y = y - 1\n",
    "\n",
    "# Ensure indices are aligned\n",
    "X, y = X.align(y, join='inner', axis=0)\n",
    "\n",
    "# Standardize features\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert X_scaled back to DataFrame\n",
    "X_scaled = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)\n",
    "\n",
    "# Perform group k-fold cross-validation using tile_id\n",
    "n_splits = 5\n",
    "groups = data['tile_id']\n",
    "if CV == 'Group':\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "else:\n",
    "    gkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"LDA\": LinearDiscriminantAnalysis(),\n",
    "    \"QDA\": QuadraticDiscriminantAnalysis(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"MLP\": MLPClassifier(),\n",
    "    \"BernoulliRBM\": Pipeline([\n",
    "        (\"rbm\", BernoulliRBM(random_state=42)),\n",
    "        (\"logistic\", LogisticRegression())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    \"Logistic Regression\": {\"C\": [0.1, 1, 10], \"penalty\": [\"l2\"], \"solver\": [\"liblinear\"]},\n",
    "    \"LDA\": {\"solver\": [\"svd\", \"lsqr\"]},\n",
    "    \"QDA\": {\"reg_param\": [0.0, 0.1, 0.5]},\n",
    "    \"KNN\": {\"n_neighbors\": [3, 5, 7], \"metric\": [\"euclidean\", \"manhattan\"]},\n",
    "    \"Random Forest\": {\"n_estimators\": [30, 50], \"max_depth\": [None, 10], \"min_samples_split\": [2, 5], \"max_features\": [\"auto\", \"sqrt\"]},\n",
    "    \"XGBoost\": {\"n_estimators\": [30, 50], \"learning_rate\": [0.01, 0.1], \"max_depth\": [3, 6], \"colsample_bytree\": [0.8, 1.0]},\n",
    "    \"MLP\": {\"hidden_layer_sizes\": [(10,), (50,), (10, 10), (50, 50)], \"activation\": [\"relu\"], \"solver\": [\"adam\"], \"alpha\": [0.0001, 0.001], \"learning_rate\": [\"constant\"]},\n",
    "    \"BernoulliRBM\": {\n",
    "        \"rbm__n_components\": [64, 128],\n",
    "        \"rbm__learning_rate\": [0.01, 0.1],\n",
    "        \"logistic__C\": [0.1, 1, 10]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Checkpoint file paths\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Results dictionary\n",
    "results = {}\n",
    "metrics_file = f\"results/algorithm_search_model_evaluation_results_{extra}.csv\"\n",
    "# regional_metrics_file = \"results/algorithm_search_regional_metrics_breakdown_{Y}Y.csv\"\n",
    "\n",
    "# Load existing metrics if they exist\n",
    "if os.path.exists(metrics_file):\n",
    "    results_df = pd.read_csv(metrics_file, index_col=0)\n",
    "    results = results_df.to_dict(orient=\"index\")\n",
    "else:\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "# if os.path.exists(regional_metrics_file):\n",
    "#     regional_metrics_df = pd.read_csv(regional_metrics_file, index_col=0)\n",
    "#     regional_metrics = regional_metrics_df.to_dict(orient=\"index\")\n",
    "# else:\n",
    "#     regional_metrics_df = pd.DataFrame()\n",
    "#     regional_metrics = {region: {} for region in data['greco_region'].unique()}\n",
    "\n",
    "# Function to evaluate model using multiple metrics\n",
    "def evaluate_model(model, X, y, cv, groups):\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    if cv.__class__ == GroupKFold:\n",
    "        for train_index, test_index in gkf.split(X, y, groups):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "            precision_scores.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "            recall_scores.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    elif cv.__class__ == StratifiedKFold:\n",
    "        for train_index, test_index in gkf.split(X, y):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "            precision_scores.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "            recall_scores.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    return {\n",
    "        'f1_score': np.mean(f1_scores),\n",
    "        'precision': np.mean(precision_scores),\n",
    "        'recall': np.mean(recall_scores),\n",
    "    }\n",
    "\n",
    "# Function to save checkpoint\n",
    "def save_checkpoint(name, search, extra=''):\n",
    "    filename = os.path.join(checkpoint_dir, f\"{name}_{extra}_checkpoint.pkl\")\n",
    "    joblib.dump(search, filename)\n",
    "\n",
    "# Function to load checkpoint\n",
    "def load_checkpoint(name, extra=''):\n",
    "    filename = os.path.join(checkpoint_dir, f\"{name}_{extra}_checkpoint.pkl\")\n",
    "    if os.path.exists(filename):\n",
    "        return joblib.load(filename)\n",
    "    return None\n",
    "\n",
    "# Perform hyperparameter optimization and evaluation for each model\n",
    "def perform_hyperparameter_search(name, model, force=False):\n",
    "    if name in results and not force:\n",
    "        print(f\"Metrics for model {name} already exist, skipping evaluation.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Processing model: {name}\")\n",
    "    param_grid = param_grids[name]\n",
    "    search = load_checkpoint(name, extra)\n",
    "    if search is None or force:\n",
    "        search = HalvingGridSearchCV(model, param_grid, cv=gkf, scoring='f1_weighted', n_jobs=-1, verbose=1)\n",
    "        search.fit(X_scaled if name in [\"Logistic Regression\", \"KNN\", \"MLP\"] else X, y, groups=groups)\n",
    "        save_checkpoint(name, search, extra)\n",
    "    else:\n",
    "        print(f\"Resuming from checkpoint for {name}\")\n",
    "        if not hasattr(search, 'best_estimator_'):\n",
    "            search.fit(X_scaled if name in [\"Logistic Regression\", \"KNN\", \"MLP\"] else X, y, groups=groups)\n",
    "    best_model = search.best_estimator_\n",
    "    metrics = evaluate_model(best_model, X_scaled if name in [\"Logistic Regression\", \"KNN\", \"MLP\"] else X, y, gkf, groups)\n",
    "    results[name] = metrics\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df.to_csv(metrics_file)\n",
    "    print(f\"Best parameters for {name}: {search.best_params_}\")\n",
    "    print(f\"Evaluation metrics for {name}: {metrics}\")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "force = True\n",
    "for name, model in models.items():\n",
    "    perform_hyperparameter_search(name, model, force=force)\n",
    "\n",
    "print(\"Model evaluation results saved to model_evaluation_results.csv\")\n",
    "\n",
    "# # Breakdown of metrics per eco region\n",
    "# for region in regional_metrics:\n",
    "#     region_indices = data[data['greco_region'] == region].index\n",
    "#     X_region = X_scaled.loc[region_indices]\n",
    "#     y_region = y.loc[region_indices]\n",
    "#     region_groups = data.loc[region_indices, 'tile_id']\n",
    "#     for name, model in models.items():\n",
    "#         if region in regional_metrics and name in regional_metrics[region] and not force:\n",
    "#             print(f\"Metrics for model {name} in region {region} already exist, skipping evaluation.\")\n",
    "#             continue\n",
    "\n",
    "#         best_model = load_checkpoint(name).best_estimator_\n",
    "#         metrics = evaluate_model(best_model, X_region, y_region, region_groups)\n",
    "#         regional_metrics[region][name] = metrics\n",
    "#         regional_metrics_df = pd.DataFrame(regional_metrics).T\n",
    "#         regional_metrics_df.to_csv(regional_metrics_file)\n",
    "\n",
    "# print(\"Regional metrics breakdown saved to regional_metrics_breakdown.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import joblib\n",
    "from utils import load_and_preprocess_table_data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier, BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Load data\n",
    "Y = 3\n",
    "config = f\"no_resample_cloud_disturbance_weights_{Y}Y\"\n",
    "CV = 'Group'\n",
    "extra = config + '_' + CV\n",
    "data = load_and_preprocess_table_data(config)\n",
    "\n",
    "# Define features and target\n",
    "features = ['amplitude_red', 'cos_phase_red','offset_red',\n",
    "            'cos_phase_blue', \n",
    "            'amplitude_crswir', 'cos_phase_crswir', 'sin_phase_crswir', 'offset_crswir', \n",
    "            'elevation']\n",
    "target = 'phen'\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "y = y - 1  # Adjust target labels to start from 0\n",
    "\n",
    "# Ensure indices are aligned\n",
    "X, y = X.align(y, join='inner', axis=0)\n",
    "\n",
    "# Standardize features\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"LDA\": LinearDiscriminantAnalysis(),\n",
    "    \"QDA\": QuadraticDiscriminantAnalysis(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"MLP\": MLPClassifier(),\n",
    "}\n",
    "\n",
    "# Checkpoint directory\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "\n",
    "# Function to load checkpoint\n",
    "def load_checkpoint(name, extra=''):\n",
    "    filename = os.path.join(checkpoint_dir, f\"{name}_{extra}_checkpoint.pkl\")\n",
    "    if os.path.exists(filename):\n",
    "        return joblib.load(filename)\n",
    "    return None\n",
    "\n",
    "# Results dictionary\n",
    "time_results = []\n",
    "\n",
    "# Measure training and inference time\n",
    "def measure_time(model_name: str, model, X, y):\n",
    "    start_time = time.time()\n",
    "    model.fit(X, y)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.predict(X)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    return training_time, inference_time\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating model: {name}\")\n",
    "    checkpoint = load_checkpoint(name, extra)\n",
    "    if checkpoint:\n",
    "        best_model = checkpoint.best_estimator_\n",
    "        X_used = X_scaled if name in [\"Logistic Regression\", \"KNN\", \"MLP\"] else X\n",
    "        train_time, infer_time = measure_time(name, best_model, X_used, y)\n",
    "        time_results.append({\n",
    "            \"model\": name,\n",
    "            \"training_time\": train_time,\n",
    "            \"inference_time\": infer_time\n",
    "        })\n",
    "        print(f\"{name}: Training time = {train_time:.4f}s, Inference time = {infer_time:.4f}s\")\n",
    "\n",
    "# Save time results\n",
    "time_results_df = pd.DataFrame(time_results)\n",
    "time_results_df.to_csv(f\"results/training_inference_times_{extra}.csv\", index=False)\n",
    "print(\"Training and inference times saved to results/training_inference_times_{extra}.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kayrros-default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
